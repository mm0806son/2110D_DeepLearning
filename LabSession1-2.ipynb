{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 1 & 2: Introduction to PyTorch and Automatic Differentiation\n",
    "\n",
    "Authors: [pierre.tandeo@imt-atlantique.fr](pierre.tandeo@imt-atlantique.fr), [lucas.drumetz@imt-atlantique.fr](lucas.drumetz@imt-atlantique.fr), [simon.benaichouche@imt-atlantique.fr](simon.benaichouche@imt-atlantique.fr), [aurelien.colin@imt-atlantique.fr](aurelien.colin@imt-atlantique.fr)\n",
    "\n",
    "Year: 2021-2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this practice is to explain $y$ (output variable) as a function of one or several $x$ (input variables) using neural networks. Here, $x$ are continuous and $y$ can be continuous (*regression*) or discrete (*classification*). These 2 aspects are treated in the deep learning framework. Both linear and nonlinear cases will be covered in this lab session.\n",
    "\n",
    "We use the PyTorch library to implement deep learning architectures. This interface has the following characteristics:\n",
    "- it is using tensors, kind of arrays that can be easily manipulated by GPUs\n",
    "- it is using the automatic differentiation to easily compute the gradient of complex functions\n",
    "- it is one of the most important deep learning library\n",
    "- it can interact with classical machine learning libraries like *Scikit-learn*\n",
    "\n",
    "This lab session is an introduction. It gives you the general concepts of neural nets and helps you to implement them using a dedicated Python library. If you want to play with similar regression and classification examples in small dimensions, you can use http://playground.tensorflow.org/. For more details concerning the implementation of PyTorch, please visit this tutorial at https://pytorch.org/tutorials/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import classic libraries (Matplotlib and PyLab)\n",
    "%matplotlib inline\n",
    "%pylab inline\n",
    "\n",
    "# Import PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Parameters (figure size and random seed)\n",
    "pylab.rcParams['figure.figsize'] = (15,15)\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression**\n",
    "\n",
    "Simple regression is a linear problem between continuous variables $x$ and $y$. Here, we write the model $y=2+0.5x$ and generate $y$ using an additional Gaussian standard random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x = torch.randn(1000, 1)*10 # input variable\n",
    "y_true = 2 + 0.5*x # true model\n",
    "y = y_true + torch.randn(1000, 1)*2 # add noise to the truth\n",
    "\n",
    "# Plot noisy data and true model\n",
    "plot(x, y_true, 'r')\n",
    "plot(x, y, 'b.')\n",
    "legend(['Model', 'Data'], prop={'size': 20})\n",
    "title('Linear regression', size=20)\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective is to build a neural net to fit the relationship between $x$ and $y$. In PyTorch, it is necessary to create a specific class for each neural network architecture. Below, we declare the neural network corresponding to a linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a class for linear regression\n",
    "class linear_regression_nn(nn.Module):\n",
    "    \n",
    "    # class initialization\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(linear_regression_nn, self).__init__()\n",
    "        # fully connected layer with linear activation\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    # function to apply the neural network\n",
    "    def forward(self, x):\n",
    "        y_pred = self.fc(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we create a neural network based on the specific architecture declared above. We finally check the values of the parameters (randomly generated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the neural network (1 input size for x and 1 output size for y, bias is declared by default)\n",
    "linear_regression_model = linear_regression_nn(1, 1)\n",
    "\n",
    "# Print the model architecture\n",
    "print(linear_regression_model)\n",
    "\n",
    "# Print the model parameters (weights of the neural network)\n",
    "slope, bias = linear_regression_model.parameters()\n",
    "print('Slope: ', slope)\n",
    "print('Bias: ', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://tandeo.files.wordpress.com/2020/09/simple_regression_dl.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to quantify the quality of the fit given by the neural network, we define a loss function. For a regression problem, the classic loss function is the Mean Squared Error (MSE). But other loss functions are already defined in PyTorch: https://pytorch.org/docs/stable/nn.html#loss-functions.\n",
    "\n",
    "Then, to compute the gradient of the loss function w.r.t. the parameters of the neural network, we use an optimization technique. A classic procedure is the Stochastic Gradient Descent (SQG). Other optimizers are coded in PyTorch: https://pytorch.org/docs/stable/optim.html#."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: MSE = sum [(y - y_pred)^2], with y_pred = w0 + w1*x\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer: new_parameters = old_parameters - lr*gradient, with lr the learning rate\n",
    "optimizer = torch.optim.SGD(linear_regression_model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit the neural network using the ($x$, $y$) dataset. At each iteration (epoch), the loss function is stored and the estimated regression line is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1000 # number of epochs\n",
    "losses = [] # list to stock the loss at each iteration\n",
    "\n",
    "# Loop on epochs\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # compute the prediction using the previous parameters of the neural network\n",
    "    y_pred = linear_regression_model.forward(x)\n",
    "    \n",
    "    # compute and stock the loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # initialize the gradient to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # compute the gradient by back propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    #Â update the parameter values using the gradient\n",
    "    optimizer.step()\n",
    "    \n",
    "    # plot the adjustment at each epoch\n",
    "    plot(x, y_true, 'r')\n",
    "    plot(x, y, 'b.')\n",
    "    plot(x, y_pred.detach().numpy(), 'g')\n",
    "    legend(['Model', 'Data', 'Successive fits'], prop={'size': 20})\n",
    "    title('Linear regression', size=20)\n",
    "    xlabel('x', size=20)\n",
    "    ylabel('y', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to check that the loss function is decreasing. If not, it maybe means that the learning rate in the optimization procedure is too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the loss function\n",
    "plot(range(epochs), losses)\n",
    "title('Loss function', size=20)\n",
    "xlabel('Epoch', size=20)\n",
    "ylabel('Loss value', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's check if the estimated parameters (at the last epoch) are close to the true ones (i.e., $0.5$ for the slope and $2$ for the bias)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and print the model parameters (weights of the neural network)\n",
    "slope, bias = linear_regression_model.parameters()\n",
    "print('Slope: ', slope)\n",
    "print('Bias: ', bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nonlinear regression**\n",
    "\n",
    "Now, we perform a multiple regression between continuous variables $x$ and $y$. Here, we write the nonlinear model $y=2+0.5x-0.05x^2$ and generate $y$ using an additional Gaussian standard random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x = torch.randn(1000, 1)*10 # input variable\n",
    "y_true = 2 + 0.5*x - 0.05*x**2 # true model\n",
    "y = y_true + torch.randn(1000, 1)*2 # add noise to the truth\n",
    "\n",
    "# Plot noisy data and true model\n",
    "plot(x, y_true, 'r.')\n",
    "plot(x, y, 'b.')\n",
    "legend(['Model', 'Data'], prop={'size': 20})\n",
    "title('Nonlinear regression', size=20)\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We declare the neural network corresponding to a nonlinear regression. The nonlinearities are introduced by a hidden layer with ReLu activations. Other activation functions are available in PyTorch: https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a class for nonlinear regression\n",
    "class nonlinear_regression_nn(nn.Module):\n",
    "    \n",
    "    # class initialization\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(nonlinear_regression_nn, self).__init__()\n",
    "        # fully connected layer with linear activation\n",
    "        self.fc0 = nn.Linear(input_size, hidden_size)\n",
    "        # ReLu activation\n",
    "        self.relu = nn.ReLU()\n",
    "        # fully connected layer with linear activation\n",
    "        self.fc1 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    # function to apply the neural network\n",
    "    def forward(self, x):\n",
    "        out = self.fc0(x)\n",
    "        out = self.relu(out)\n",
    "        y_pred = self.fc1(out)\n",
    "        return y_pred\n",
    "    \n",
    "# Create the neural network (1 input size for x, 6 neurons in the hidden layer, and 1 output size for y)\n",
    "nonlinear_regression_model = nonlinear_regression_nn(1, 6, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can check the structure of the network and see the parameter vales (randomly generated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model parameters (weights of the neural network)\n",
    "for name, param in nonlinear_regression_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://tandeo.files.wordpress.com/2020/09/multiple_regression_dl.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still use the SGD optimzer and MSE loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function: MSE = sum [(y - y_pred)^2]\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer: new_parameters = old_parameters - lr*gradient, with lr the learning rate\n",
    "optimizer = torch.optim.SGD(nonlinear_regression_model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we fit the neural network using the ($x$, $y$) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20000 # number of epochs\n",
    "losses = [] # list to stock the loss at each iteration\n",
    "\n",
    "# Loop on epochs\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # compute the prediction using the previous parameters of the neural network\n",
    "    y_pred = nonlinear_regression_model.forward(x)\n",
    "    \n",
    "    # compute and stock the loss\n",
    "    loss = criterion(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # initialize the gradient to zero\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # compute the gradient by back propagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # update the parameter values using the gradient\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function is then plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the loss function\n",
    "plot(range(epochs), losses)\n",
    "title('Loss function', size=20)\n",
    "xlabel('Epoch', size=20)\n",
    "ylabel('Loss value', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the estimated model (last iteration of the neural network) is plotted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the adjustment at the last epoch\n",
    "plot(x, y_true, 'r.')\n",
    "plot(x, y, 'b.')\n",
    "plot(x, y_pred.detach().numpy(), 'g.')\n",
    "legend(['Model', 'Data', 'Fit'], prop={'size': 20})\n",
    "title('Nonlinear regression', size=20)\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression (binary classification)**\n",
    "\n",
    "In a classification problem, $y$ is a discrete variable with various classes. Here, $y$ is binary and takes its values between $0$ and $1$. We write the model as $y=f\\left(2+0.5 x\\right)$ with $f$ the logistic transfer function (sigmoid). As previously, we generate $y$ adding Gaussian perturbations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x = torch.randn(1000, 1)*10 # input variable\n",
    "y_true = 1 / (1 + exp(-(2 + 0.5*x))) # true model\n",
    "y_noise = 1 / (1 + exp(-(2 + 0.5*x + torch.randn(1000, 1)*2))) # add noise to the truth\n",
    "y = (y_noise>0.5).float() # transform to binary data\n",
    "\n",
    "# Plot noisy data and true model\n",
    "plot(x, y_true, 'r.')\n",
    "plot(x, y, 'b.')\n",
    "legend(['Model', 'Data'], prop={'size': 20})\n",
    "title('Logistic regression', size=20)\n",
    "xlabel('x', size=20)\n",
    "ylabel('y', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your own neural network to fit the data generated below. Be careful, the logistic regression is a classification problem. The output $y$ is binary. You thus have to use appropriate activation and loss functions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://tandeo.files.wordpress.com/2020/09/logistic_regression_dl.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automatic differentiation**\n",
    "\n",
    "A neural network is based on the minimization of a cost function (also called loss function) w.r.t. to some parameters, these parameters corresponding to the weights of the network. The loss function measures the adequacy between the observed data and the model (i.e., the neural network).\n",
    "\n",
    "In deep learning architectures, loss functions are complicated, corresponding to a succession of several activation functions (e.g., linear, ReLU, sigmoid). Minimizing this function is thus tricky. The minimization procedure is performed using gradient descent algorithms, but computing gradients of complicated loss function is not straightforward. \n",
    "\n",
    "Deep learning libraries like PyTorch use automatic differentiation to compute efficiently and rapidly the gradient of the loss function at each epoch. The goal here is to implement a minimization problem using automatic differentiation and compare it with a classic method based on the exact calculation of the derivative. Then, the second objective will be to implement, by hand, the iterations of the optimization of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a sigmoid function, well known is neural networks, especially for classification problems: tanh. The exact derivative of tanh is also defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tanh function\n",
    "def f(x):\n",
    "    return (torch.tanh(x))\n",
    "\n",
    "# Define the first derivative of tanh\n",
    "def f_prime(x):\n",
    "    return (1 - torch.tanh(x)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we compare the analytic solution to the approximation computed by automatic differentiation in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "x = torch.linspace(-5.0, 5.0, requires_grad=True)\n",
    "f_x = f(x)\n",
    "f_prime_x = f_prime(x)\n",
    "\n",
    "# Apply the automatic differentiation\n",
    "y = torch.sum(f(x))\n",
    "y.backward(retain_graph=True)\n",
    "x.grad\n",
    "\n",
    "# Plot results\n",
    "subplot(2,1,1)\n",
    "plot(x.detach().numpy(), f_x.detach().numpy(), 'b')\n",
    "plot(x.detach().numpy(), f_prime_x.detach().numpy(), 'g')\n",
    "plot(x.detach().numpy(), x.grad.detach().numpy(), 'r--', linewidth=5)\n",
    "plot(array([-5, 5]), array([0, 0]), 'k--')\n",
    "legend(['f(x) = tanh(x)', 'f prime (analytic)', 'f prime (automatic)'], prop={'size': 20})\n",
    "title('Tanh and its first derivative', size=20)\n",
    "xlabel('x', size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take an example of classification with several inputs $x_1, x_2, x_1^2, x_2^2, x_1 x_2$ (resulting in nonlinearities) and a binary output $y$. We define a true model with 6 parameters $w$, including the bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input variables\n",
    "x1 = torch.randn(100, 1)\n",
    "x2 = torch.randn(100, 1)\n",
    "\n",
    "# True parameters\n",
    "w0 = -0.5\n",
    "w1 = 1.5\n",
    "w2 = 0.5\n",
    "w11 = 0.75\n",
    "w22 = 0.25\n",
    "w12 = 0.5\n",
    "\n",
    "# Noise\n",
    "epsilon = torch.randn(100, 1)/3\n",
    "\n",
    "# Output variable\n",
    "y_noise = torch.tanh(w0 + w1*x1 + w2*x2 + w11*x1**2 + w22*x2**2 + w12*x1*x2 + epsilon)\n",
    "y = (y_noise>0.5).float() # transform to binary data\n",
    "\n",
    "# For visualization\n",
    "X1, X2 = torch.meshgrid(torch.linspace(-4, 4, 100), torch.linspace(-4, 4, 100))\n",
    "Y = torch.tanh(w0 + w1*X1 + w2*X2 + w11*X1**2 + w22*X2**2 + w12*X1*X2)\n",
    "\n",
    "# Plot data\n",
    "scatter(x1[y==0], x2[y==0], c='b')\n",
    "scatter(x1[y==1], x2[y==1], c='r')\n",
    "contourf(X1, X2, Y, cmap='bwr', alpha=0.5)\n",
    "scatter(x1, x2, c=y, cmap='bwr')\n",
    "xlabel(\"$x_1$\", fontsize=20)\n",
    "ylabel(\"$x_2$\", fontsize=20)\n",
    "title('Nonlinear classification', size=20)\n",
    "legend(['y = 0', 'y = 1'], prop={'size': 20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, define the loss function for such a binary classification. A classic one is the BCE defined as $\\sum_{i=1}^n \\left[ - y_i \\log(\\hat{y}_i) - (1 - y_i) \\log(1 - \\hat{y}_i) \\right]$, with $\\hat{y}$ the prediction of the model defined by the 6 parameters $w$ and the input variables $x_1, \\dots, x_1 x_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we use the *autograd* module of PyTorch to compute automatically the gradient of the loss function defined before. As an example, we evaluate for instance the gradient when $w$ parameters are randomly sampled using a Gaussian distribution with mean $0$ and variance $1/n$, with $n$ the number of parameters. The parameter $w_0$, corresponding to the bias, is initialize with zero. More details about the initialization of a neural network are discussed here: https://www.deeplearning.ai/ai-notes/initialization/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "# Define initial parameter values\n",
    "w = Variable(torch.cat((torch.zeros(1), torch.randn(5)*sqrt(1/6))), requires_grad = True)\n",
    "\n",
    "# compute the loss\n",
    "loss = torch.sum(loss_function(w))\n",
    "\n",
    "# Apply backpropagation\n",
    "loss.backward(retain_graph=True)\n",
    "\n",
    "# Compute the gradient of the loss function evaluated at w\n",
    "gradient = w.grad\n",
    "\n",
    "# Print results\n",
    "print('Loss: ', loss)\n",
    "print('Gradient: ', gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdly, implement your own gradient descent algorithm to iteratively update the parameters $w$ of your model. Plot the results of your classification model. Then, verify values of the estimated parameters. There are maybe several solutions of this nonlinear classification problem. Thus, the estimated parameters may be different from the real ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, implement this classification using a neural network defined using PyTorch, as in the first examples (linear, nonlinear and logistic regressions) of this practice. We suggest to implement a MLP (multilayer perceptron) with at least 2 hidden layers. The input layer must be only $x_1$ and $x_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://tandeo.files.wordpress.com/2020/09/classification_dl.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "### TO DO ###\n",
    "#############"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "description": "Use Caffe as a generic SGD optimizer to train logistic regression on non-image HDF5 data.",
  "example_name": "Off-the-shelf SGD for classification",
  "include_in_docs": true,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "priority": 3
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
