{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE2F5Cj5X-cq"
   },
   "source": [
    "# Lab session 4 | Convolutional Neural Networks (CNN) with pytorch\n",
    "\n",
    "pierre-henri.conze@imt-atlantique.fr \\\\\n",
    "francois.rousseau@imt-atlantique.fr \\\\\n",
    "simon.benaichouche@imt-atlantique.fr \\\\\n",
    "aurelien.colin@imt-atlantique.fr\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZNAqdzCX-cs"
   },
   "source": [
    "## Objective of this lab session: perform classification on MNIST using convolutional neural networks\n",
    "\n",
    "In lab session 3, MNIST classification has been performed relying on Multi-Layer Perceptron (MLP) models. The obtained accuracy was 92% with a softmax regressor and 97% (or 98%) with a deeper MLP. This can be further improved! Let us jump from such simple models to something moderately more sophisticated, namely **Convolutional Neural Networks** (CNN).\n",
    "\n",
    "For recall, **MNIST** is a computer vision dataset which consists of handwritten digit images with associated label. Each image in MNIST has a corresponding label - a number between 0 and 9 - representing the digit drawn in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85qr0BEYX-cu"
   },
   "source": [
    "### 1- Data management\n",
    "\n",
    "Start with these lines of code to automatically download the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "W6fsgjPOX-cv"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/homes/a19sella/.local/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.ToTensor() # convert data to torch.FloatTensor\n",
    "\n",
    "train_data = datasets.MNIST(root = 'data', train = True, download = True, transform = transform)\n",
    "test_data = datasets.MNIST(root = 'data', train = False, download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQKjqImJ4mKx"
   },
   "source": [
    "#### **Question 1.1** - Complete the following cell to create **data loaders** ([documentation](https://pytorch.org/docs/stable/data.html)) for training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ho-I57ync1Ya"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import numpy as np\n",
    "\n",
    "batch_size = 20 # how many samples per batch to load\n",
    "valid_size = 0.2 # percentage of training set to use as validation\n",
    "\n",
    "def create_data_loaders(batch_size, valid_size, train_data, test_data):\n",
    "    num_train, num_test = len(train_data), len(test_data)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size)\n",
    "    # obtain training indices that will be used for validation\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size * num_train))\n",
    "    train_index, valid_index = indices[split:], indices[:split]\n",
    "\n",
    "    # define samplers for obtaining training and validation batches\n",
    "    train_sampler = SubsetRandomSampler(train_index)\n",
    "    valid_sampler = SubsetRandomSampler(valid_index)\n",
    "\n",
    "    # prepare data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = valid_sampler)\n",
    "    return train_loader, valid_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "gXWfzusFrlmQ"
   },
   "outputs": [],
   "source": [
    "train_loader, valid_loader, test_loader = create_data_loaders(batch_size, valid_size, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18G47IQQX-c3"
   },
   "source": [
    "Let us visualize some images from the training set with corresponding ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "executionInfo": {
     "elapsed": 2019,
     "status": "ok",
     "timestamp": 1601150749171,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "arAFBf_q0K3p",
    "outputId": "bda651dd-5b73-4afe-ea7a-919340592e39"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABXEAAAD7CAYAAAAsAtcsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3defxV8/b48fWuNA+a1dUgIWWIiksUKQmppDQrcr9CrjI2mSmiboqKUAoXzSQN0q24lAYiaaB5UGhOqfb3j9y399o6x/mczjl7n895PR+PHr+1fuvss9f9tttnn7e91zGe5wkAAAAAAAAAIJxyBN0AAAAAAAAAACAyFnEBAAAAAAAAIMRYxAUAAAAAAACAEGMRFwAAAAAAAABCjEVcAAAAAAAAAAgxFnEBAAAAAAAAIMRYxAUAAAAAAACAEMuoRVxjTCtjzLfGmL3GmNXGmEuD7gnhZYy50xjzhTHmgDFmZND9IH0YY840xswyxuw0xqwyxjQLuieEmzEmjzHmFWPMWmPMbmPMYmNMo6D7QrgZY/b4/hw2xgwOui+EG9c3yCo+oxAPPqMQL2PMGGPMZmPMLmPMCmNM56B7Qrhl0rVNrqAbSBVjTAMReVpEbhSR+SJSJtiOkAY2icgTItJQRPIF3AvShDEml4hMEpFhItJAROqKyHvGmPM8z1sRaHMIs1wisl6OHi/rRORqEXnHGHO253lrgmwM4eV5XsH/xcaYAiKyVUTeDa4jpAmub5BVfEYhy/iMwnHoKyK3eJ53wBhTRURmG2MWe563MOjGEFoZc22TSXfiPioij3me95nneUc8z9voed7GoJtCeHmeN97zvIki8lPQvSCtVBGRsiIy0PO8w57nzRKRT0SkfbBtIcw8z9vred4jnuet+f0z6n0R+UFEagTdG9LGDSLyo4jMDboRhBvXN8gqPqOQAHxGIWae533jed6B/6W//zk1wJYQcpl0bZMRi7jGmJwiUlNESv7+aPMGY8wQY0y2XqEHEAgT4f/vrFQ3gvRljCktIqeLyDdB94K0cZOIvO55nhd0IwCyNz6jEAc+o5AlxpgXjTH7RGS5iGwWkQ8CbgkIhYxYxBWR0iJyghz9L4CXikh1ETlPRHoH2RSAbGm5HL3T4D5jzAnGmCvl6OOH+YNtC+nCGHOCiLwhIqM8z1sedD8IP2NMeTl6nhkVdC8Asjc+o5BVfEYhHp7n3S4iheTo+s14ETkQfQsgM2TKIu7+3//fwZ7nbfY8b7uIDJCj85wAIGE8z/tNRJqKyDUiskVE7hGRd0RkQ5B9IT0YY3KIyGgROSgidwbcDtJHBxGZ53neD0E3AiD74jMKceIzCnH5fTTdPBE5WUS6BN0PEAYZsYjred4vcnQBhcc3ACSd53lfeZ5X1/O84p7nNRSRSnL0BxWBiIwxRkRekaNPjzT//T8IALHoINzhBCCJ+IzCceAzCscrlzATFxCRDFnE/d1rItLVGFPKGFNURO4WkfcD7gkhZozJZYzJKyI5RSSnMSavMSZX0H0h/Iwx5/x+vOQ3xtwrImVEZGTAbSH8horImSLS2PO8/X/1YkBExBhzsYj8TfjFb8SI6xvEic8oZBmfUciq39drWhljChpjchpjGopIaxGZFXRvCK9MurbJpEXcx0VkgYisEJFvRWSxiDwZaEcIu95ydBTHgyLS7veYOcqIRXs5OoD/RxG5QkQaOL+wCvyJMaaCiPyfHJ3ZvsUYs+f3P20Dbg3hd5OIjPc8b3fQjSBtcH2DLOEzCseBzyhklSdHRydsEJFfRORZEbnb87xJgXaFsMuYaxvDD0QCAAAAAAAAQHhl0p24AAAAAAAAAJB2WMQFAAAAAAAAgBBjERcAAAAAAAAAQoxFXAAAAAAAAAAIMRZxAQAAAAAAACDEcmXlxcYYL1mNIMu2e55XMugmYsFxEx6e55mge4gFx0yocK5BPDhuEA+OG8SD4wbx4LhBPDhukGV8B0ccIp5ruBM3fa0NugEAGYFzDeLBcYN4cNwgHhw3iAfHDeLBcQMgFSKea1jEBQAAAAAAAIAQYxEXAAAAAAAAAEKMRVwAAAAAAAAACDEWcQEAAAAAAAAgxFjEBQAAAAAAAIAQYxEXAAAAAAAAAEKMRVwAAAAAAAAACDEWcQEAAAAAAAAgxFjEBQAAAAAAAIAQyxV0A8lQoUIFld977702bt26taoVK1ZM5Z7n2fikk05StW3btiWqRWSQCRMm2LhZs2YBdgIAAAAAAIB0xJ24AAAAAAAAABBiLOICAAAAAAAAQIhlm3EK5cqVs/EHH3ygameccUbE7dzxCSIi69evt/GBAwcS1B0yScuWLVX+xRdfBNQJAADINO51cMOGDVUtR44/7t+4+eabVe21115LbmMAAAA4LtyJCwAAAAAAAAAhxiIuAAAAAAAAAIQYi7gAAAAAAAAAEGJpOxM3Vy7derdu3WwcbQau37p161TeqFEjG+/atSvO7pBpLr/8chuPGjVK1UaMGJHqdgBkuMKFC6s8Z86cNj506JCq7d69OyU9AUiOunXrqvySSy6xsf+3H44cOWLjQYMGqdrq1atVPmfOnES1iDRTsWJFlbvXubVr1466bfny5W1cv379iK/r16+fyh9//HEb79+/P5Y2ASTBWWedpfIhQ4bY+O23305pLy+++KLKV65cqfKBAwdG3HbNmjU2njp1akL7Qvb0zjvv2LhFixaq5q4N1qtXT9UWLlyY3MZ8uBMXAAAAAAAAAEKMRVwAAAAAAAAACLG0HafQtWtXld91110xbbdgwQKVt2vXTuX+R8mAWJQoUcLGM2fOVLVLL7001e0gIO4jrPfff7+qNW7cWOXuI66ffPKJqr333ns2fu6551Tt8OHDx90n0kfp0qVVfsMNN9i4QIECqtawYUMbX3DBBarmvvaXX35RNfdzkMfNgPSzbNkylX/77bc2rlGjRsTt8ufPr3J3NJkI4xSyo2rVqtn4+uuvV7XWrVvbuGTJkqpWrFgxGxtjVM0/siPW2oMPPqjyRYsW2Xjs2LERtwOQXP7vru73GzdOBXcEkIhIpUqVVD548OCI27rjwpYsWaJq7du3t/GGDRuOp0WksS5duqi8WbNmNvYfewULFrRxrVq1VI1xCgAAAAAAAAAAi0VcAAAAAAAAAAgxFnEBAAAAAAAAIMTSaibuWWedZeNevXrF9R633HKLypmBi0S48cYbbdy5c2dV69SpU6rbQYrUrFlT5a+//rqNK1asqGrR5sLVrl07Yl6hQgVVu+OOO7LaJkLOPVbuvvtuVXNndomIFC1aNOL77Nixw8ZvvPGGqhUqVMjG7txDEZFWrVrZmJm48XOvUWbNmqVq7tx0vzFjxtjYnWWaKm+99ZaN16xZk/L94/ht27ZN5cOGDbPxyy+/HPP7fP311wnrCcFxz/fPPvusqt166602jnZdEgR3fjMzcVPPvb7IkydPxNfVrVtX5dHmbkcTbbayO/9f5M/Xwq4cOf64J23r1q2q5l6Xv/TSS6q2atWq2JvNMDNmzFC5e21y6qmnqlru3LltvGvXLlWLdo655557bHz77bermruPO++8M2JNRKRnz57H7EVEnwv9c37ffvttG19xxRWq9uuvv0bsG+nv/PPPt/GgQYNUzT2f+G3atMnGr7zySuIbywLuxAUAAAAAAACAEGMRFwAAAAAAAABCLK3GKfTu3dvGxYoVUzX3dv3Dhw+rWo8ePWy8bNmyJHWHTLZx40Ybd+nSRdUGDBiQ6naQQAUKFFC5+4jXkCFDor7WtXPnTpXnzJnTxu5j8CIiJ598so394zkmT55s42nTpkXcH8KlZcuWNnbHr4iI1KtXz8Z79uxRtc8++0zl7iOmixcvVjX3McLNmzerWp8+fSL2tmTJkog1xM59HLB48eKqFu2RwrZt28a1P/dR1ON5LPqmm26y8eOPP65q/rEcSA9z586Na7t58+YluBMEwX1MuGHDhjFvt2HDBhv7P4tWrlxp408++UTV/I8ev/nmmzYuUqSIqrmPaZ9yyikx94bEu+2221TujiosU6ZMxO2ijUHIiqy8T7TakSNHbOwfXdS9e3cblypVStU6duwYS5sZyT9q4uyzz7ax/xrWHSXlH9/i/+4TyciRI2PurVy5cipv1qyZjc8999yY32fSpEk2ZnxCZnHPde73cb/du3er3D1n/PbbbwnvKyu4ExcAAAAAAAAAQoxFXAAAAAAAAAAIMRZxAQAAAAAAACDE0mombrVq1Wzsn43j5p9//rmqPffcc8ltDBmnQ4cOKm/durWNTzzxRFV78sknU9ITkuP5559XuTsPxz/P67333rOxO7tWJPq8J//5rFKlSjZetGiRqt155502ZiZu+rjqqqtsvHTpUlXr1q2bjf3zkXft2pWQ/efKFfnj/tChQwnZR6Zr3Lhx0C3E5bTTTrPxq6++qmoHDhywsTuPGUB41axZM6bXLViwQOVXX321jX/++eeE9FKwYEGV+3/TBMG56KKLVB5tDm4yHDx4UOWzZ8+O633y5Mlj4zp16kR8XdWqVVXu/o7F3r1749p3JnJnbh8rT7Q1a9aoPHfu3Cr3zzqOxP+dadiwYcfVF9KHf/53o0aNYtpu+vTpKv/oo48S1tPx4k5cAAAAAAAAAAgxFnEBAAAAAAAAIMRCPU6hfv36Kj/99NMjvtZ95I/H15Fs559/vspLlChh4ylTpqjakSNHUtITEqd69eo2btmypar98MMPNnYfgxcRmTlzpo33798fdR/t2rWz8ddff61qS5YssfH69etj6Bhhd/PNN6d0f/fee6/Ke/ToYWP/Y/EvvPBCSnrK7rp3727jESNGqJr/keKw8o/dyJcvX0CdIAj+Rw4Z2ZOe2rRpY+OHH3444uvatm2r8kSNUHBVqFBB5UWKFLGxfySVP0dy9e7dW+XuY+v+vzeX/+/JPxLMNWrUKBtv27ZN1fzfj5YtWxbxfaKJNq7MNW7cOJUzQiFY5cqVs/E999yjau5YOfd1IiL79u1T+bp162zs/8xyR9stXLhQ1RI1rgzhc9JJJ6n8jjvuULk7gsXPPS7+9a9/JbaxBOJOXAAAAAAAAAAIMRZxAQAAAAAAACDEWMQFAAAAAAAAgBAL3Uzc3Llz27hnz56qljNnzojbvffeezb+8MMP495/qVKlbFy1atWYt1u5cqWNN27cGPf+EU758+dXeZ06dSK+tnPnzio/fPhwUnpC4px11lkqd+ca+2fbXn755TbOyrxa/z5ee+01G7tzdkX0/G9m4iJWV1xxhY3985rdY8w/F4653Ynx7rvv2jhHDv3fyG+55Za43tOd9//oo4/GvJ07A1lEpFChQjauVauWqhUuXDiu3hBe27dvt/GcOXNUrW7duhG3q1KlStJ6Quq48/ndOFXceYPutY6Inp/622+/qdo777yT3Mag+K8vo81PDrP27dvb2D+fd+3atTZ+/fXXU9YT/uyqq65SuXstGm1Gqd9DDz2k8oEDBx5fY8gWypQpY2P/Z0lW1vTc6+dPP/30+BtLEu7EBQAAAAAAAIAQYxEXAAAAAAAAAEIsdOMUbrvtNhtHe2Td79///ndc+/Pv4+mnn7ax/5HDaLZu3Wrjt956S9XuvffeuHpDsMqXL2/jN954Q9WqV6+u8lmzZtl4x44dyW0MCeE+ujN48GBVcx/J8D/+E+94g0ceeUTl7niYypUrq9oll1xi41y59Gl6woQJce0f2U+FChVU7o4S8o8fch83fPvtt5PbGP70f+NU/9+8efPmEWuff/65ymvWrJnsdpBi7nXI0qVLVc0dCeQfpeIfAwLEwv8odO/evW1csWLFiNv17dtX5YsWLUpoX8g+3O/r7sgzEX3e8p/TlixZYuPNmzcnqTtEUq5cORv7R3lFG6GwYMECGzds2FDV9u7dm6DukJ2ce+65Nr744otj3m7Xrl0q/+qrrxLWUzJxtQYAAAAAAAAAIcYiLgAAAAAAAACEGIu4AAAAAAAAABBioZuJW6xYMRsbYyK+bvjw4SqfOHFixNcWKFDAxmPGjFG1pk2bqtw/SydWZcuWtXG3bt1U7aWXXrLxihUr4np/BMs/s9Rv9uzZNj5w4ECSu0EidOrUycZ169ZVtfHjx9t4xowZcb1/4cKFVX7NNdfEvK07z/T5559XtalTp8bVD7KHjh072vj2229XNfe4efTRR1XtzTffTGpfCLdChQrZOG/evAF2glTzPE/l7nWuv+b+FoCInqU9evToJHSH7OCBBx5Qec+ePSO+dubMmTbu169f0npC+smXL5+NGzRooGqvvvrqMV8nInLw4EEbu9/HRETuvPPOBHaIrDp06JCN/bNHS5YsGXE7dz3o/fffVzX/51a8Jk+ebONp06ap2q+//mrjVatWJWR/SC73d7X+ys6dO218yy23qNqnn36asJ6SiTtxAQAAAAAAACDEWMQFAAAAAAAAgBAL3TiFatWq2Tja7fJLly6NWDvxxBNVPmvWLBufc845quYfn+Du0/9Y/HvvvWfjhg0bqpr76LS/71atWtn4sccei9g3wsV9rNA/ruOXX35Ref/+/VPSExLHfaTYfdxHRKR37942jvexnX/84x8qz5MnT1zv4z7ug+DlyPHHf/v0/51WrVrVxmvWrFE193Np06ZNqnb48GEbu48Fivx5LMfTTz9t41KlSqnaiBEjbOwfp4DMVr9+fRufddZZAXaCVBs2bJjKu3btGvG1J5xwgsqLFCmSlJ6Q3po3b67y++67L+Jrd+zYofI77rjDxu4jy8g8/msod3yYO/Lsr7jjO5577rnjbwwJs3nzZhu/9tprqnb//fdH3O7UU0+1sX+kYaLGKdSuXdvG7rW1iF5nOu+88xKyPyTWTTfdpPJ69erFvO3KlSttPGHChIT1lErciQsAAAAAAAAAIcYiLgAAAAAAAACEGIu4AAAAAAAAABBigc/EdedSiojkz5//uN/nlVdeUTX/HFyXO4tQROTzzz+3sTvLVkRk48aNNp4/f76q1ahRI+I+3Lk+zMRNH507d7Zxhw4dVM2djyzy5/nJCL9rr73WxsuWLVO15cuXx/We7vzAli1bxtcYAlewYEEb9+jRQ9UuuugiG1922WUJ2d+WLVts/NJLL6las2bNVF6yZEkbf/fdd6o2fvx4GxctWjTi/vwzoHfv3h17s0gL/tmmTZo0CagTBC3ezzMRkeuvv97GQ4YMSUQ7SFOXXHKJjceOHatq0WZUdu/eXeWrVq1KbGNIW8OHD1d5u3btIr72+++/t7H/O9lnn32W2MaQFO7s4mPlrtatW9v4rbfeUjX3d0tERGbPnn3M7f7KhRdeaGP/3NsSJUrY2P29JhGRb775JuZ9IHn8ny0FChSIeVv/fOZ0xJ24AAAAAAAAABBiLOICAAAAAAAAQIixiAsAAAAAAAAAIRb4TFx35oiIyMUXXxzX+zRu3NjGWZn95p93+Nxzz8W0nX+WRrSZuP65vwinFi1aqLx9+/Y2NsaoWv/+/VPSE5LHne/Wr1+/hLznE088YeOaNWuqmn/+do4cf/w3NP/xhdRyZ+CKiEyePNnGtWrVilhr3ry5qu3YsSPiPkqVKmXja665RtXcGV4PP/ywqkWbNXj66aerfMqUKRFf6/rll19UvmTJEhu/+OKLqjZu3LiY3hPh4p8L536eRbNt2zaVL168OGE9IRzcz54jR45EfW3dunWT3Q5CqmzZsiofPHiwjf2fS3v37lX5/fffb+MxY8YkoTuko1dffVXl/s+laNc7zz//vI2ZgZv9+efgutzvWn7z5s2LWPOvx7i/QVG9enVVK1y4sI0rV66saszEDY57bVuxYsWYt5sxY4bK/XPd0xF34gIAAAAAAABAiLGICwAAAAAAAAAhFvg4hQ0bNqh84cKFNr7ssssiblenTh2Vx3pLdZ8+fVQe6/gEZE+5cv3xT6BXr16q5j7ivnr1alXz50g/CxYssHHHjh1V7dlnn7Xxnj17Ir7HPffco/IuXbrY2D8+oWfPnip3x3f4Ry8gtQYMGKDyv//97zb2fw7Nnz8/rn2UL1/exn379lW1nDlz2vjQoUOq1rJlS5Vv3rw54j5OPvlkG/sfb61Xr17E7UqXLm3jZ555RtUYp5Ce/MdtrCNbxo8fr/Kvv/46US0hJB599FEb9+7dO8BOEDZlypSx8Ycffqhq1apVs/FPP/2kaoMGDVL50KFDk9Ad0kHRokVVPnDgQBv7xye4o11ERA4ePGhj/zXzkCFDEtUiMtQDDzyg8jZt2kR87ffff2/jSZMmJa0nROe/lh0+fLiNCxQoEHE7/3d3/7XO9u3bj7+5gHEnLgAAAAAAAACEGIu4AAAAAAAAABBiLOICAAAAAAAAQIgFPhPXb/LkyTa+/PLLI77uxhtvVLnneTb2z6IcM2aMjf2zCONVt25dlbtzfY4cOaJqsc6iQ+o1bdrUxuecc46qHThwwMbujFSR6HMpkR7y5ctnY3deqYjIsmXLbDxq1ChVK1GihI3dubYi+jzw2WefqdrHH3+s8n79+mWxYyTLRRddpPK33nrLxtFm4ObJk0flNWrUsPFdd92las2aNbOxe24R0ceYf7bgkiVLIu4/K6ZOnZqQ90E4+T+/3M82EX2N5Of+FoH/uEX2459nGquSJUuqfNu2bYloByHizvV3Z+D6+a9vnnjiiaT1hPBzr6E/+eQTVXPnLPs/h/xzK93jz/3uDsSrcuXKNvb/xgTCr3r16iqPNgfX/S2QO+64Q9W++OKLxDYWAtyJCwAAAAAAAAAhxiIuAAAAAAAAAIRY6MYp/Pvf/7bxgAEDYt7OfUTD/6jYzTffHFcvefPmVfltt91m46uvvlrV3BEK/sdFJk6cGNf+kXwPP/xwxNr3339v42HDhqWiHaTQ2LFjbXzeeeep2sknn2zjXr16xfyes2fPtrH7+LyIHt8gIrJ06VIbV61aVdVWrVoV8z6ReLly/fHRWKhQIVUrW7asjf3HRrt27SK+5/r162185ZVXqtp3330XV5/A/0yZMkXlRYoUiXlb9zGzQ4cOJawnZC/Tp09XeYMGDWy8ffv2VLeDBKhZs6bK3cfZ/VasWGHjzp07J60nhF/RokVV7o6gcscn/JWnnnpK5R999NHxNYaM5D5y36lTJ1Vzx2+64/D8pk2bpvInn3wyQd0hq9zvXd27d495u7lz59o4E8axcCcuAAAAAAAAAIQYi7gAAAAAAAAAEGIs4gIAAAAAAABAiIVuJq47V6tGjRqq5s6WLV++fMT38M882bJlS8TX+medVqpUycZXXXWVqhUrVizi+7gmT56s8nvuuSem7ZB6FStWDLoFBKRfv3423rFjh6r179/fxjlz5lS1mTNn2njOnDmqNnDgQBsfPnxY1dw5uyIi1apVs/F///tfVdu4cWPU3pFYuXPnVnn79u1t7M59FNGz4PLkyaNqH3/8sY1Hjx6tauPHj7fxrl274m8WOIbChQvH/Npt27ap/MUXX0x0OwgxY8wx47/izh0UEXnttdds3Lhx4+NvDCnhnivc3wYQESlevHjE7dzPsMqVK6ua/zcA3Fm77sxtEf17AP75/1u3bo24f4THrbfeqvILL7wwpu2WLVum8r59+yasJ2Rf/t+m8M/yHjVqlI3d3634K+53P3d2rojInj17stIiEuiNN96w8d/+9reYt9u9e3cy2gkt7sQFAAAAAAAAgBBjERcAAAAAAAAAQix04xSOHDli4y+//FLV6tevb+O77rpL1dq1a2fjIkWKqJp/vIKrT58+Kvc8L/ZmHUOGDLHx7NmzVW3v3r1xvSeA1PCPVXnzzTdt7H/cdOfOnXHtw/84kDumoWTJknG9JxKjXr16Kv/www9t7I69EBHZv3+/jf/5z3+q2tChQ2186NChRLYI/EnHjh1tnD9//pi369Kli8q//vrrRLWENOBe52blmte9Pkf6ckcolCtXTtWiHQ9t2rSxcY8ePWLerlOnThFrX331lcqvuOIKG//8888Rt0PquX9Xp5xySszbud/d/aM1kJ7y5s2r8rp169p42rRpCd/fO++8o3L/mLNYjRw5UuUjRoywMeMTgpMvXz6V+8dlRHLw4EGVu2MSMwF34gIAAAAAAABAiLGICwAAAAAAAAAhxiIuAAAAAAAAAIRY6GbiRrN69Wob+2cRDhw40MbubBaR6LM16tSpo3J3/uGCBQtUbfLkyTaeNGmSqi1btiziPpCeBg0aFHQLCMiuXbtSur/t27endH/QNm7cqPKLLrrIxrly6Y9Jdy5kqo8TZDb//MrevXvbOEeO2P+b/Pr16xPWE9KPey17//33q1qZMmUibrdjxw6Vu78FgfThzijNykzk8uXLJ7yXc889V+WtW7e28QsvvJDw/SG6008/3cbPPfecqrnfj/2fN+5syp49e6qa/3dikP7853733+3ChQtVzf2tiCZNmqjalVdeGdP+/L91FO289c0336jc/Y0T/zHNb1eEw9tvv63y0qVLx7Sdfy1wyZIlCespHXAnLgAAAAAAAACEGIu4AAAAAAAAABBiaTVOIZo1a9YcMxYRGTVqVGqbQdooVKhQ0C0A8vPPPwfdAhx79uwJugXgT7p06aLyU0891cbumI9j+fzzz23sjqZC5lm7dq2NR48erWr+8Qpjx4618Ysvvqhqc+bMSUJ3SAfLly9XeVbGMpx44ok23rlzp6ox6iW1/CMy7rvvPhs3atRI1dy/Y3d8goge7eN/ZB3Zz+DBg1Veo0YNG/tHWPo/Y+KxdOlSlfvPNyNHjrTxxIkTVc39vEM4uSPs/op7rfvpp58mo520wZ24AAAAAAAAABBiLOICAAAAAAAAQIixiAsAAAAAAAAAIZZtZuICQLpauHBh0C0AyEY++eQTlT/00EM2/uWXX1LdDkKqZ8+eUXNkP++//76Nr7nmGlWbOXOmjcePH69q7lzK45lFWLp0aRtv3bo17vfB8Xv88cdV3rZt25i2858nmIObWb788kuVn3feeTbu27evqrlzlv3zamfMmBHT/oYPH57VFpFG3M8dEZGWLVvaeMCAAarmfvZ8/fXXyW0s5LgTFwAAAAAAAABCjEVcAAAAAAAAAGZr5qoAACAASURBVAgxxikAQADcRxOffvrpADsBkN2sXr1a5bNnzw6mEQChct111wW6f0YoBKt48eI29o/TiKZ+/fo25vMEkfTo0SNqDvi1bt06ao5j405cAAAAAAAAAAgxFnEBAAAAAAAAIMRYxAUAAAAAAACAEGMmLgCkwGeffabyc889N6BOAKSj6dOnq/y8886z8ZVXXhn1tQAA7Nixw8b+32Po27dvxO2WLVuWtJ4AAFnDnbgAAAAAAAAAEGIs4gIAAAAAAABAiBnP82J/sTGxvxjJttDzvJpBNxELjpvw8DzPBN1DLDhmQoVzDeLBcYN4cNwgHhw3iAfHDeLBcYMs4zs44hDxXMOduAAAAAAAAAAQYiziAgAAAAAAAECIsYgLAAAAAAAAACGWK4uv3y4ia5PRCLKsQtANZAHHTThwzCAeHDeIB8cN4sFxg3hw3CAeHDeIB8cNsopjBvGIeNxk6YfNAAAAAAAAAACpxTgFAAAAAAAAAAgxFnEBAAAAAAAAIMRYxAUAAAAAAACAEMu4RVxjzGnGmF+NMWOC7gXhZoy50xjzhTHmgDFmZND9ID0YY/IYY14xxqw1xuw2xiw2xjQKui+EG+cbxMMYs8f357AxZnDQfSHcjDFjjDGbjTG7jDErjDGdg+4J4WeMqWiM+cAY84sxZosxZogxJqs/ko0Mw/kG8eC4QTwy5bjJuB82M8ZMF5F8IrLW87x2QfeD8DLGXC8iR0SkoYjk8zyvY7AdIR0YYwqIyH0iMlJE1onI1SLyloic7XnemuA6Q5hxvsHx+v3cs1VErvY8b07Q/SC8jDHVRGSV53kHjDFVRGS2iFzjed7CYDtDmBljPhCRH0XkNhE5UURmiMjLnuc9H2hjCDXON4gHxw3ikSnHTUbdiWuMaSUiO0Tko6B7Qfh5njfe87yJIvJT0L0gfXiet9fzvEc8z1vjed4Rz/PeF5EfRKRG0L0hvDjfIAFukKMLLHODbgTh5nneN57nHfhf+vufUwNsCenhFBF5x/O8Xz3P2yIiH4pItYB7QshxvkE8OG4Qj0w5bjJmEdcYU1hEHhORe4LuBUDmMMaUFpHTReSboHsBkK3dJCKve5n2iBXiYox50RizT0SWi8hmEfkg4JYQfoNEpJUxJr8x5m8i0kiOLuQCUXG+QTw4bhCPTDhuMmYRV0QeF5FXPM9bH3QjADKDMeYEEXlDREZ5nrc86H4AZE/GmPIiUldERgXdC9KD53m3i0ghEblURMaLyIHoWwDyHzl65+0uEdkgIl+IyMRAO0Ja4HyDeHDcIB6ZcNxkxCKuMaa6iNQXkYFB9wIgMxhjcojIaBE5KCJ3BtwOgOytg4jM8zzvh6AbQfrwPO+w53nzRORkEekSdD8Ir9+vaabJ0S/EBUSkhIgUFZGng+wL6YPzDeLBcYN4ZPfjJiMWcUXkMhGpKCLrjDFbROReEWlujFkUZFMAsidjjBGRV0SktIg09zzvt4BbApC9dRDuwkX8ckk2nBmHhComIuVEZIjneQc8z/tJRF6Toz/eCmQF5xvEg+MG8ciWx02mLOK+JEf/8qr//meYiEyRo78CDhyTMSaXMSaviOQUkZzGmLzGmFxB94W0MFREzhSRxp7n7Q+6GYQf5xvEyxhzsYj8TUTeDboXhJ8xppQxppUxpqAxJqcxpqGItBaRWUH3hvDyPG+7HP2R1i6/f16dKEfncH8ZbGcIM843iAfHDeKRScdNRiziep63z/O8Lf/7IyJ7RORXz/O2Bd0bQq23iOwXkQdFpN3vce9AO0LoGWMqiMj/ydH/YLTFGLPn9z9tA24N4cb5BvG6SUTGe563O+hGkBY8Ofpo4QYR+UVEnhWRuz3PmxRoV0gH14vIVSKyTURWicghEekWaEcIO843iAfHDeKRMceN4UeMAQAAAAAAACC8MuJOXAAAAAAAAABIVyziAgAAAAAAAECIsYgLAAAAAAAAACHGIi4AAAAAAAAAhFiurLzYGMOvoIXHds/zSgbdRCw4bsLD8zwTdA+x4JgJFc41iAfHDeLBcYN4cNwgHhw3iAfHDbKM7+CIQ8RzDXfipq+1QTcAICNwrkE8OG4QD44bxIPjBvHguEE8OG4ApELEcw2LuAAAAAAAAAAQYiziAgAAAAAAAECIsYgLAAAAAAAAACHGIi4AAAAAAAAAhBiLuAAAAAAAAAAQYiziAgAAAAAAAECIsYgLAAAAAAAAACHGIi4AAAAAAAAAhBiLuAAAAAAAAAAQYrmCbgAAsqMXXnhB5bfffrvKGzdubOP3338/JT0BAAAAAID0xJ24AAAAAAAAABBiLOICAAAAAAAAQIgxTgEAEuSkk06y8TnnnKNqnuepvFChQinpCQCQ/ooUKaLyadOm2fiCCy6IuJ0xRuVLliyx8bJly1StQYMGKp80aZKNb7311tibBZCRatSoofIzzzzTxs2aNVO1pk2bqtw9V/mvmV9++WUbz5s3T9XGjBkTX7MAsqWiRYuq/JZbblF5//79bbx//35Va9KkiY1nzJiRhO4SgztxAQAAAAAAACDEWMQFAAAAAAAAgBBjERcAAAAAAAAAQoyZuD6DBw+28WWXXaZqZ599doq7Qbpo1KiRjSdOnKhqDz74oI0HDhyYsp6QfLlz51b5O++8Y+OLL7446rYrV660ccWKFVVtzZo1x90bwmv79u0qX7x4sY07d+6samvXrk1JTwDCLW/evCqPNgfX5Z8f6c6sPPfcc6Nue/PNN9v4yJEjqtalS5eINQQrV64/vt658/1ERB566CEb+7/XuDNJ33rrLVUrU6aMjefOnatqI0eOVLl7DcOxkf2ULFlS5e6s26FDh6qaO9vWP5/bP/c2Ws2dye2fpXv++efb+KmnnlI1//UWjl+ePHlUfvXVV9vYP1fd/X7s/67jfj8eNGiQqv3666/H2yYyjHtN9Mwzz6japZdeqnL3c8l/PL/99ts2/vbbbyPub+zYsSpP9RoPd+ICAAAAAAAAQIixiAsAAAAAAAAAIWaiPcrwpxcbE/uL09TSpUttXKpUKVUrXbp0qtuJZqHneTWDbiIWmXDcjBs3zsbuY0UiIj/++KONTzrppJT1dCye55m/flXw0uWY8f9d+x+tcEV7jMz/2NC7775rY/+jYStWrMhyn8eJc02CzZ8/X+W1atWysf/vu1evXinpKQky+rgpXry4yrt27WrjunXrqpr7+Kn/3/eSJUsS3VrYZfRxE02BAgVU7o4zmDNnjqpt2LDBxlu2bFE191HoQoUKqdq1116r8gEDBsTUz/79+yO+LkUy+ri57rrrVP7www/buHr16one3V964IEHbPzss8+mfP9ZkNHHTbyuuuoqlU+ZMsXG0a51/TX/o8r79u2zcYkSJVStQoUKx3xPEZEcOf64J23ZsmWqVq1atT//Dzh+GX3cdOvWTeWx/hv/+uuvVe6OEuvZs6eqbdq0Kc7uwovv4InVsWNHlQ8fPtzG7kihrHKvmdxzkohIpUqVbOw/1yRp7GrEcw134gIAAAAAAABAiLGICwAAAAAAAAAhxiIuAAAAAAAAAIRY/AMjApA7d24bu3MvRPQslc2bN8f8nv6ZGSeccEKc3SGT+GfTXXLJJRFf6585h/TmzgJ7/fXXY97ujTfeUPkNN9xg47x586pa+/btbezO+hIR6dChQ8z7RDg1btxY5R9//LGN77rrLlXzz7McPHhw8hpDwsycOVPl7qws/1zASy+91Mb+65cnnnjCxv7rHmSWvXv3qjzeWaNbt249ZiwicuWVV0bcbsGCBSo/dOhQXPtHYrjnkUaNGqmaOwf3yJEjqubO4F+3bp2qvfTSSxH317ZtWxuffvrpqua/zv373/8e8X2Q/po2baryaL+vM378eBtPnDhR1SZMmKDyaDNxy5cvb+NRo0apWtWqVW18xhlnqJr72xX+/SE+7nzirLjppptUnoEz/5FA69evV3nOnDkjvtY/j/lf//qXjf2fg+7vY+3cuVPVli9fnuU+k4U7cQEAAAAAAAAgxFjEBQAAAAAAAIAQS6txCsWLF7fxtddeq2pdu3aN6z3Lli2rcvcxjLlz58b1nsj+6tSpo/KSJUtGfK3/sR+ktzFjxtjYP1bj4MGDNvaP2Pjiiy9U7o5MeOSRR1TtoYcesnG7du1UjXEK6c//CPOMGTNs7B+nUKlSpZT0hONXsWJFG/sfBf3Pf/5j41NOOUXV3MdEy5Qpo2q9e/e28cknn6xqffr0ibtXQEQfeyIiVapUUbk7wuH//u//VO23335LXmP4S+74t9NOOy3i6zZt2qRydyzCvHnzYt7foEGDbFyuXDlV838n++GHH2J6z/z586u8SJEiNvZ/TvrHQiA427ZtU7k72qNLly6qFm1ERzTbt2+PmNerV0/V3M9X/zgFdwzRwoULVc3/GDVi434PEhFp1aqVjaN9H3ZHa4iIrF692sb+f98rVqyw8bJly1TNf/y5j9V//vnnEfeP7OWjjz5S+UknnRTxte6olmPlkdStW1flpUuXtrH/MyrVuBMXAAAAAAAAAEKMRVwAAAAAAAAACDEWcQEAAAAAAAAgxNJqJq47K65gwYKq5s6m3LNnT8zveeWVV0asjRs3LgvdIZM8+OCDMb+W4yh7GTt2rI3dOWAiIv3797fxqlWr4t6H53kRa+7czTVr1sS9D4THm2++aWP/TFx3fqGISLdu3VLSE7LO/fd4+eWXR6y5/4ZF9N+/fyaum/fo0UPVrrjiChv7Z2d///33MfWMzNOwYUMbv/vuu6rmn0vYtGlTGy9ZsiS5jSFL3Bn8c+bMUbWaNWva2D9L+/HHH7fxfffdp2r+2f2RuDMoRUSGDh0a03Z+bdq0Ubk7v7RTp06q9vrrr8e1DyTed999p3L3mtU/9zRR/vGPf9j41ltvVTV3Dq7/+tmdn+qfs4v4+M8TV111lY0fffRRVTv//PNtXKxYMVWrUKFCxH3Ur18/5n7cc6H/nDJ16lQb//rrrzG/J9JPov59N27c2MZPPfWUquXJkydiLdW4ExcAAAAAAAAAQoxFXAAAAAAAAAAIsbQap+A+LpE3b15Vq1Klio23bt0a83ueeeaZEWubNm3KQnfIzooWLary8uXLR3zt2rVrVR7r42lID7fddltK9/fVV1+pnBEK2Y/7mfXjjz+qmn9kB9JDtHEq/po7esH/KKD76HOJEiVU7cILL7Sx/3PG/7irO9ZnwYIFqvaf//wnYq9ITyeddJKN7777blW74447bOyOIhMRueSSS1T+6aefJqE7JNpjjz2m8s8++8zG7uPEIiJ16tSxce3atVXN/fxZt25dIlvMsssuu0zlY8aMsbF/7AdSy/27OFbucr+f+89F/u/g3377bcTapZdeamP/yIR9+/bZeMKECarm5u7rkDjuqJ0mTZpEfF3lypVV3rVr14i1woUL2/jiiy+Ouv/cuXPb2D/a5ZxzzrEx4xRwLP6xQg899JCN8+fPr2r33HOPjT/88MPkNvYXuBMXAAAAAAAAAEKMRVwAAAAAAAAACDEWcQEAAAAAAAAgxNJ2Jm6inHbaaRFrM2fOTPj+kJ5atWql8goVKkR87bBhw1S+c+fOpPSE9ObOVW7RokXE1y1dujQV7SBApUuXtnGpUqVUbdu2baluBym2d+9eG7/88suq5s4kbdSokaq5c7z883IvuOCCqLnLnet1ww03qNr+/fsjbofw6tu3r41vuummmLdr2bKlylesWGHj7du3H39jSInFixfb2P07FBE5/fTTbTxgwABVc+f/ucdQIrmfcc2aNYv4umuvvVbl7m+hMNs0XHr16mXjpk2bqpo7E9c/X9I/29adye2vubm/5s697dChQ6xtI8X8vwfwz3/+M+Jr3WPFPy/X/xsAOXPmtPGOHTtUzf87E8hM/t/SevTRR2185513RnztlClTVG3ixIk2PnjwYCJbzDLuxAUAAAAAAACAEGMRFwAAAAAAAABCLK3GKVx44YU2/u2331Rt/fr1cb1noUKFItaCvk0a4fHggw/G/Novv/wyiZ0gXdWuXVvlr7zyio39Y10OHDhg4zfffDO5jQEIrW+++eaYsYjICy+8YOPmzZurmj+vXr26jcuVK6dqV111VcTtxowZk8WOEQZPPvmkjf2Pnl533XU2vvjii1XtrrvuUnnHjh1t7B+1MH369ONtE0nijuGpUaOGqo0YMcLGN954o6q5x0atWrVUbfjw4TaeNm1azL0UK1ZM5ePGjbOxe17yc3sRYYRCmLRr107ljz32mI2NMarmjj7w1/yi1d2af7TLU089FfV9EX7uGo+Ifty9QYMGUbd1z3fudsgs7jnCPxrsr3KXO0Khe/fuqrZmzZrj6DCxuBMXAAAAAAAAAEKMRVwAAAAAAAAACDEWcQEAAAAAAAAgxEI9E7dKlSoqr1evno39M3HXrVtn45w5c6ra4cOHbVy1alVVq1u3rsp/+umnY24nIpIrV66INXfmD7KHv/3tbzYuUaJE1NfOnz/fxrNmzUpaTwg3d75gkyZNVO3+++9XuXvOWLlypaq5cwmzMnsOQObYv3+/jf2za/15mTJlbLx06VJVK1q0qI2ZiZs9rFq16pixiJ6l3LlzZ1W76KKLVO7OjXNnmYqI1K9f38aff/55/M0iqfyzZG+99VYbjx49WtUqV65s43/961+qVqFCBRv/97//VbVdu3apvEWLFjaO9psSH330kcrdubuLFi2KuB2C1bRpU5VH+w6c6hrCa+bMmSp3r0sqVaqkarlz5474PpMmTVJ5165dbbxx48bjaRFppGTJkiqP9jsRfjt27LCx/zPq5ZdfTkB3yceduAAAAAAAAAAQYiziAgAAAAAAAECIhXqcwsMPP6xy99Z6/2327niFbdu2qdqSJUtsXLx4cVUzxqjcvb36mWeeUTX3dv0uXbqo2rBhw/78PwBp7eqrr7Zxvnz5or524MCBNj548GDSekLw+vbta+NWrVqpWtmyZW3sjl8R+fPjXx9++KGNO3bsqGo//vjj8baJNFK6dOmgW0A2t3nzZhtH+4xyH5lG9jdixIio+YYNG2zcp08fVRsyZIiNr7nmGlXjMyy89u7da+OpU6eqWuHChW3cpk0bVbvgggts7H8s+v3331d59+7dbVyoUCFVO+OMM2x86NAhVVuzZk201hES/u/A06dPj/ja8ePH23j79u1R37dXr1429o9sqFGjho39j1G7j04/+eSTUfeB1CpfvryNq1evrmruKKdo/KN8xo4dq3JGKGRf1apVU/l1111nY3c0kEjWrl/d9b/zzz9f1SpWrGjjMH8mcScuAAAAAAAAAIQYi7gAAAAAAAAAEGIs4gIAAAAAAABAiIV6Jm40s2fPVrk7461UqVKq1qBBg5jf99RTT7WxOwNXRGT16tU2dufsInvwz7296667Yt523759iW4HIVG3bl2V33PPPTbOmTOnqh0+fNjGK1euVDX/+WTWrFnH3A6Zp0WLFkG3AIiIyIIFC4JuASHy6KOP2rh27dqqVq9ePRv757r7f1MC6WHXrl02fuSRR1RtzJgxNnbnkx4rj1WY5w0iMv9vz7z00ksJeV93nq1/Dqqbu3OVj5UjPNatW2fjkSNHqlq3bt1ieg935rHIn2ew33vvvTbes2ePqrm/YzJjxoyY9ofweOihh1R+ww03xLTd2rVrVe7OghfR8987dOigasOHD89Ki4HhTlwAAAAAAAAACDEWcQEAAAAAAAAgxFjEBQAAAAAAAIAQC/VM3GeffVblw4YNs/GcOXNUzfM8GxtjVC1Hjj/Wqr/77jtVc2fgiuj5uR9//HHEfRw5ciRq70g/BQoUUHm1atUivtadGyYi8t577yWlJwQvf/78Knfn4O7evVvVWrdubeOpU6cmtzFkGy+//LKN27Vrp2qfffZZqttBNtS+fXsbly5dWtXca6S5c+emrCf8tbJly9r4ueeeU7W2bdvaOFnXpO77jh07VtUuu+wyG/tnFvbv39/G7rUz0se0adNU7v5OhDsfF0iG5cuXq9z9PYpRo0apWsmSJW1cokQJVdu+fXsSukM87r//fpU///zzNm7SpImquX+P7sxbEZG8efOqPNpM7jp16tjYPxe1Z8+eNn7hhRcivgeCM336dJW7M3H9taefftrGixcvVrWdO3eq3D3exo8fr2oNGza0cZh/A4s7cQEAAAAAAAAgxFjEBQAAAAAAAIAQC/U4hYULF8a1nf/RrcOHD0es+W3ZsuWY2yH7q1WrVsyv7datWxI7QZhUrFgxYq1QoUIq/+mnn5LcDbKjAwcORKwtWLAghZ0gu/Cfm5o1a2Zj/3XQjh07bDxv3rzkNoYsccf53Hjjjao2YsQIG3/00UdJ78UdaSYiUrNmTRvffPPNqlapUiUbr169OrmNISUaNWqUkPepUqWKjVetWpWQ90T2V7t2bRsXL15c1RjZkh78Y3/WrVtn48GDB0fcbtKkSSovWLCgyq+77jobV65cWdUaN24ccbvHHnvMxt98842qzZ49O2I/SJ1XX31V5e5Ypz179qhatHW7XLn0kuftt98e8bXucRlm3IkLAAAAAAAAACHGIi4AAAAAAAAAhBiLuAAAAAAAAAAQYqGeiZsoRYoUsXHhwoVVzT/Dcv369SnpCeGQM2dOG99www0xb7d48eJktIMQuu+++1RujLHxxo0bVW3FihUJ2ac7h/fMM89UtW3bttn4iy++SMj+AGQvF110kcrdmXF+o0ePtvGaNWuS1RLi4J+DGyYXXnihjf2fRT/88EOq20GCtW7dWuVNmzaN+Nq2bduqvE+fPjZ2Z+CKiLz++us29p+X3N9C2b9/f+zNIqIKFSqofP78+TZ2r2dF9NzjeH+X5ngUKFDAxu4cdxGR5s2b29jf9/Lly228ffv2JHWHoCxatChqfc6cOTbOkUPfn3j22Wfb+P3331e1smXL2tidjysiUqdOnSz3icTzz7veuXNnTNudfPLJKn/llVdUXr9+fRv/+uuvqvbtt99mpcXAcCcuAAAAAAAAAIQYi7gAAAAAAAAAEGIZMU7h+uuvt3GpUqVUbenSpSqP9TZtZA81atSwcadOnSK+zv+IKY+cZm8lSpSwsTvaQEQ/2uE+iiMiMmjQIBt/8MEHquYfy7BgwQIb58mTR9VatGhh4/z586vaunXrbFytWjVV27dvnwBA+/btY36te05BuLgjn4J2zjnnqPyMM86wsf9x1yNHjqSkJyRPmzZtVO4+6u7nH58xYMAAG7/00kuq5o64e/jhh1XN/b6GxLj00ktVXrx4cRv7xxJMmTLFxpdddpmquSMLEsU/asMdy9GjRw9Vc3sdP368qj311FMJ7w3pyf/Zs3XrVhv7R2243+E+/fTT5DaGhDjhhBNsfOKJJ6raqFGjbOyOezrWa13Tp09X+XfffXc8LaYMd+ICAAAAAAAAQIixiAsAAAAAAAAAIcYiLgAAAAAAAACEWEbMxHXnL/n95z//SWEnCDP/bChX//79Vb5jx45kt4MU8s+d/fDDD+N6n3bt2h0zFvnz8XXeeefF9J67d+9WuTu36cCBA1ltEUA25J+P3bx584iv/eKLL1Q+dOjQpPSE4zdu3DgbP/DAA6rWq1cvG8+fP1/V/J8b8apbt66Nn376aVXLleuPrxDz5s1LyP6QPbz11ls23rRpk6q9+eabNq5Xr56quXN3E3UMZ7q9e/eqfP/+/TYuWLCgqpUsWdLG3377raq556J+/fqpmvtbEf4Z6+4MXv/+/DNx3XOa+54i+jcf3GNI5M+zTpG5/HPkK1eufMzYj99ECo7/t2fcv6cbb7xR1U455RQbX3755THvY9u2bSq/4447bOz/DRv3HBlm3IkLAAAAAAAAACHGIi4AAAAAAAAAhFhGjFOIZsaMGUG3gACdfvrpNvY/uuN64403UtEOApIvXz6VRxt1sGjRIhs/+OCDqnbvvffauGbNmjHv/6uvvlL52LFjbTx9+nRVW716dczvCyAz+B+1z5MnT8TXPv744yr3P26L8Pjmm29sPGrUKFW77bbbbDx58mRV69Onj42/++47VXMfNz3nnHNU7YYbblB5y5YtbVy4cGFVmzJlio179Ohx7P8BSCvu4/QVKlSI+33cR9+nTp2qaj/++KON/cfU3XffbWP/9RXi89NPP6ncHT3gHyXmfg86cuSIqjVt2tTGzZo1i7jd+vXrVa1EiRIx7c+f+2sdOnSw8YQJEwQ4lr///e8qjzY28+eff7ax//MVx9agQQOVu9cQt9xyi6q5I5fcaxkRPQKsaNGiqlasWLG4etu4caON27Rpo2obNmxQ+Zo1a+LaR5hwJy4AAAAAAAAAhBiLuAAAAAAAAAAQYiziAgAAAAAAAECIZfxMXGQ2/xxB17Zt22x86NChVLSDEHJn7IiItG7d2sarVq1StY8++iglPQGAK9psQb8FCxYkux0kgTsvVETPuuzevbuqzZkzx8Y7d+5UNXdect68eVXt4MGDKv/2229t7M5qFxHp27evjf3zM5Ge3GOjQIECMW934403qrxjx442btGihaoVKVIk4vssW7Ys5n0iNu65QESkUaNGNh49erSqValSxcYFCxZUNfffuDEm4v78s5Tdz6Jo24no34Bo3769qrmzfJH9PPXUUyofN26cjZcsWaJq/s+tV155xcb+ma2uX375ReXnnnuujTdt2hR7sxnMP2v2+uuvt7H/nOE69dRT49rfli1bVD5//nwbjx8/XtXc3y/KhGsS7sQFAAAAAAAAgBBjERcAAAAAAAAAQoxxCsgo1113ncqrVatmY/+t9/369bPxvn37ktsYAuU+lioikjNnzoA6AbTy5csH3QKyAXfUy44dOwLsBPHyjzro06ePjYcOHapq11xzjY0HDx6sau71jPvIqsifRwKNHDkyrl6RnjZs2GDjJ554QtXc483//KJtdAAAAuFJREFUyPw///nPuPb31VdfqXzChAlxvQ9it3z5chvXqlVL1V5//XUbN2zYUNXcsQglS5aMWPNzH3meOHGiqrnjWkREFi1aFPF9kL2deeaZKncfm//ggw9UzR0JIhJ9TMfixYtt/Mwzz6gaIxSyrlOnTip3R+BUrFgxrvf0X9u89tprNvZ/P/ePOMxk3IkLAAAAAAAAACHGIi4AAAAAAAAAhBiLuAAAAAAAAAAQYszERUbZunWryg8fPmxj/5zAgQMHpqQnAJntyy+/tPGYMWNUrUmTJiq/9dZbU9ITspdZs2bZ2D9/DOnPP9vv5ZdfPmYMxMqdSygiMnr0aBv7Z1CedtppKh8yZIiN586dq2ovvviijatWrapqu3fvjq9ZJESHDh2CbgEZqkePHio/dOiQja+//vqY32fPnj0q7969u43nzJkTZ3eIpH///kG3kLG4ExcAAAAAAAAAQoxFXAAAAAAAAAAIMeN5XuwvNib2FyPZFnqeVzPoJmLBcRMenueZv35V8DhmQoVzDeLBcZNCXbt2VXnt2rVV3q1bNxtv3rw5JT3FieMG8eC4QTw4bhAPjhtkGd/BEYeI5xruxAUAAAAAAACAEGMRFwAAAAAAAABCjEVcAAAAAAAAAAixXEE3AAAAgPgNHjw4ag4AAAAg/XEnLgAAAAAAAACEGIu4AAAAAAAAABBiLOICAAAAAAAAQIixiAsAAAAAAAAAIcYiLgAAAAAAAACEGIu4AAAAAAAAABBiubL4+u0isjYZjSDLKgTdQBZw3IQDxwziwXGDeHDcIB4cN4gHxw3iwXGDeHDcIKs4ZhCPiMeN8TwvlY0AAAAAAAAAALKAcQoAAAAAAAAAEGIs4gIAAAAAAABAiLGICwAAAAAAAAAhxiIuAAAAAAAAAIQYi7gAAAAAAAAAEGIs4gIAAAAAAABAiLGICwAAAAAAAAAhxiIuAAAAAAAAAIQYi7gAAAAAAAAAEGL/D5v2wThfALeQAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1800x288 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "data_iter = iter(train_loader) \n",
    "images, labels = data_iter.next() # obtain one batch from the train set\n",
    "images = images.numpy()\n",
    "# plot images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray') # .npsqueeze removes single-dimensional entries from the shape of an array\n",
    "    ax.set_title(str(labels[idx].item())) # .item() gets the value contained in a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9vqCd8jpJnE"
   },
   "source": [
    "### 2- Simple CNN architecture\n",
    "\n",
    "Let us define a simple **CNN architecture**. The network will take as inputs 28x28 images instead of 784-dimensional tensors of pixel values as for MLP models. As in lab session 3, it will produce as output a tensor of length 10 (i.e. the number of classes) that indicates the class scores for each input image. A CNN architecture is a stack of layers including:\n",
    "  - convolutional layer using *nn.Conv2D* ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html))\n",
    "  - max-pooling layer using *nn.MaxPool2D* ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html))\n",
    "  - regular densely-connected layer using *nn.Linear* ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "897aqt3UyfHz"
   },
   "source": [
    "#### **Question 2.1** - In this first network architecture (*Net1*), employ:\n",
    " - 2 consecutive convolutional layers using 32 3x3 filters with stride 1 followed by *ReLU* activation,\n",
    " - a max pooling with vertical and horizontal downscale of 2,\n",
    " - a flatten operator to flatten the input array,\n",
    " - a dense layer with 10 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "4lLiWMnC5jBy"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net1(nn.Module):  \n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net1,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,32,3)\n",
    "        self.conv2 = nn.Conv2d(32,32,3)\n",
    "        self.rel1  = nn.ReLU()\n",
    "        self.rel2  = nn.ReLU()\n",
    "        self.pool  = nn.MaxPool2d(2,2)\n",
    "        self.fc    = nn.Linear(32*32,10)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.rel1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.rel2(out)\n",
    "        out = self.pool(out)\n",
    "        out = out.view(-1,32*32) \n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 2210,
     "status": "ok",
     "timestamp": 1601150749537,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "OFwVrc_XT3cs",
    "outputId": "da238c36-0e52-400b-a0d5-8ce00297fa57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device '+str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K48q4mnBKBx_"
   },
   "source": [
    "**torchsummary** provides information complementary to what is provided by *print(your_model)* in PyTorch, similar to the Tensorflow *model.summary()* routine ([documentation](https://pypi.org/project/torch-summary/)). This can be helpful while debugging your network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OG8wQhudOvOE"
   },
   "source": [
    "#### **Question 2.2** - Describe input/output sizes of each layer. Confirm your analysis by using *summary()* from *torchsummary*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "executionInfo": {
     "elapsed": 6470,
     "status": "ok",
     "timestamp": 1601150753817,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "JbdKfY3h-wAy",
    "outputId": "f08d9a8e-4d36-45ad-9293-3d61375341cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "├─Conv2d: 1-1                            [-1, 32, 26, 26]          320\n",
      "├─ReLU: 1-2                              [-1, 32, 26, 26]          --\n",
      "├─Conv2d: 1-3                            [-1, 32, 24, 24]          9,248\n",
      "├─ReLU: 1-4                              [-1, 32, 24, 24]          --\n",
      "├─MaxPool2d: 1-5                         [-1, 32, 12, 12]          --\n",
      "├─Linear: 1-6                            [-1, 10]                  10,250\n",
      "==========================================================================================\n",
      "Total params: 19,818\n",
      "Trainable params: 19,818\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (M): 5.51\n",
      "==========================================================================================\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.31\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 0.38\n",
      "==========================================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "├─Conv2d: 1-1                            [-1, 32, 26, 26]          320\n",
       "├─ReLU: 1-2                              [-1, 32, 26, 26]          --\n",
       "├─Conv2d: 1-3                            [-1, 32, 24, 24]          9,248\n",
       "├─ReLU: 1-4                              [-1, 32, 24, 24]          --\n",
       "├─MaxPool2d: 1-5                         [-1, 32, 12, 12]          --\n",
       "├─Linear: 1-6                            [-1, 10]                  10,250\n",
       "==========================================================================================\n",
       "Total params: 19,818\n",
       "Trainable params: 19,818\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 5.51\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.31\n",
       "Params size (MB): 0.08\n",
       "Estimated Total Size (MB): 0.38\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "cnn_1 = Net1() # initialize the neural network\n",
    "cnn_1.to(device=device)\n",
    "\n",
    "summary(cnn_1, (1, 28, 28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWkV71ZLI2ZO"
   },
   "source": [
    "#### **Question 2.3** - Before training a model, configure the learning process by indicating the criterion (i.e the objective loss function the model will try to minimize) as well as the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BeZr3uLL58Qw"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "z_IkVZB-01tJ"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(cnn_1.parameters(),lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxYFf5sG0_XC"
   },
   "source": [
    "### 3- Training\n",
    "\n",
    "#### **Question 3.1** - Complete the following cell to perform the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "TbGZO4wt6jTR"
   },
   "outputs": [],
   "source": [
    "n_epochs = 20 # number of epochs to train the model\n",
    "\n",
    "def training(n_epochs, train_loader, valid_loader, model, criterion, optimizer):\n",
    "\n",
    "  train_losses, valid_losses = [], []\n",
    "  # initialize tracker for minimum validation loss\n",
    "  valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "      train_loss, valid_loss = 0, 0 # monitor losses\n",
    "      \n",
    "      # train the model\n",
    "      model.train() # prep model for training\n",
    "      for data, label in train_loader:\n",
    "          data = data.to(device=device, dtype=torch.float32)\n",
    "          label = label.to(device=device, dtype=torch.long)\n",
    "          optimizer.zero_grad()\n",
    "          output = model(data) # forward pass: compute predicted outputs by passing inputs to the model\n",
    "          loss = criterion(output, label) # calculate the loss\n",
    "          loss.backward() # backward pass: compute gradient of the loss with respect to model parameters\n",
    "          optimizer.step()      \n",
    "          train_loss += loss.item() * data.size(0) # update running training loss\n",
    "      \n",
    "      # validate the model\n",
    "      model.eval()\n",
    "      for data, label in valid_loader:\n",
    "          data = data.to(device=device, dtype=torch.float32)\n",
    "          label = label.to(device=device, dtype=torch.long)\n",
    "          with torch.no_grad():\n",
    "              output = model(data)\n",
    "          loss = criterion(output,label)\n",
    "          valid_loss += loss.item() * data.size(0)\n",
    "      \n",
    "      # calculate average loss over an epoch\n",
    "      train_loss /= len(train_loader.sampler)\n",
    "      valid_loss /= len(valid_loader.sampler)\n",
    "      train_losses.append(train_loss)\n",
    "      valid_losses.append(valid_loss)\n",
    "      \n",
    "      print('epoch: {} \\ttraining Loss: {:.6f} \\tvalidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n",
    "\n",
    "      # save model if validation loss has decreased\n",
    "      if valid_loss <= valid_loss_min:\n",
    "          print('validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "          valid_loss_min,\n",
    "          valid_loss))\n",
    "          torch.save(model.state_dict(), 'model.pt')\n",
    "          valid_loss_min = valid_loss\n",
    "      \n",
    "  return train_losses, valid_losses      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "executionInfo": {
     "elapsed": 180302,
     "status": "ok",
     "timestamp": 1601150927690,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "fulupe2f71cJ",
    "outputId": "f3c9fc23-7732-4a43-bc29-e4223b1e83e1"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (90) to match target batch_size (20).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-ad5aba1f805c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_losses_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_losses_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcnn_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-54f5fbdb6136>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(n_epochs, train_loader, valid_loader, model, criterion, optimizer)\u001b[0m\n\u001b[1;32m     17\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m           \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# forward pass: compute predicted outputs by passing inputs to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m           \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# calculate the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# backward pass: compute gradient of the loss with respect to model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1120\u001b[0;31m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[1;32m   1121\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2822\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2823\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2824\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2826\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (90) to match target batch_size (20)."
     ]
    }
   ],
   "source": [
    "train_losses_1, valid_losses_1 = training(n_epochs, train_loader, valid_loader, cnn_1, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twmMCMSz1NoN"
   },
   "source": [
    "To study the **convergence** of the training process, we plot the evolution of the loss function for training and validation sets with respect to epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 180281,
     "status": "ok",
     "timestamp": 1601150927691,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "HpT1Y7cNBA5F",
    "outputId": "2c57e90a-9ff2-4770-fd31-a634544a2de9"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(n_epochs), train_losses_1)\n",
    "plt.plot(range(n_epochs), valid_losses_1)\n",
    "plt.legend(['train', 'validation'], prop={'size': 10})\n",
    "plt.title('loss function', size=10)\n",
    "plt.xlabel('epoch', size=10)\n",
    "plt.ylabel('loss value', size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1C17bsUC1W_Y"
   },
   "source": [
    "Let us load the model corresponding to the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 180263,
     "status": "ok",
     "timestamp": 1601150927692,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "qzToZJN3FBbS",
    "outputId": "9957d43f-b7ae-43db-96d1-d21b0d52f459"
   },
   "outputs": [],
   "source": [
    "cnn_1.load_state_dict(torch.load('model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NM5BWPs_3Pbz"
   },
   "source": [
    "### 4- Testing\n",
    "\n",
    "#### **Question 4.1** - Complete the following cell to test the (best) model on previously unseen test data and evaluate its performance through per-class and global accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQIsKly4D6bw"
   },
   "outputs": [],
   "source": [
    "def evaluation(model, test_loader, criterion):\n",
    "\n",
    "  # initialize lists to monitor test loss and accuracy\n",
    "  test_loss = 0.0\n",
    "  class_correct = list(0. for i in range(10))\n",
    "  class_total = list(0. for i in range(10))\n",
    "\n",
    "  model.eval() # prep model for evaluation\n",
    "  for data, label in test_loader:\n",
    "      data = data.to(device=device, dtype=torch.float32)\n",
    "      label = label.to(device=device, dtype=torch.long)\n",
    "      with torch.no_grad():\n",
    "          output = ... # forward pass: compute predicted outputs by passing inputs to the model\n",
    "      loss = criterion( ...  )\n",
    "    \n",
    "      test_loss += loss.item()*data.size(0)\n",
    "      _, pred = torch.max(output, 1) # convert output probabilities to predicted class\n",
    "      correct = np.squeeze(pred.eq(label.data.view_as(pred))) # compare predictions to true label\n",
    "      # calculate test accuracy for each object class\n",
    "      for i in range(len(label)):\n",
    "          digit = label.data[i]\n",
    "          class_correct[digit] += correct[i].item()\n",
    "          class_total[digit] += 1\n",
    "\n",
    "  # calculate and print avg test loss\n",
    "  test_loss = test_loss/len(test_loader.sampler)\n",
    "  print('test Loss: {:.6f}\\n'.format(test_loss))\n",
    "  for i in range(10):\n",
    "      print('test accuracy of %1s: %2d%% (%2d/%2d)' % (str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "  print('\\ntest accuracy (overall): %2.2f%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 182130,
     "status": "ok",
     "timestamp": 1601150929600,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "Dc3tr9mEZ2rP",
    "outputId": "9f7a8a20-fbd8-4a46-bb1a-4e7387d8834f"
   },
   "outputs": [],
   "source": [
    "evaluation( ... ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6Qr8y1JrplH"
   },
   "source": [
    "#### **Question 4.2** - What is the overall accuracy achieved for test data? **Answer** : ... %."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Brj2Tz_6QMU"
   },
   "source": [
    "### 5- Assessment\n",
    "\n",
    "The following cell displays test images and their labels in this format: predicted (ground-truth). The text will be green for accurately classified examples and red for incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rqBNImBD-E0"
   },
   "outputs": [],
   "source": [
    "def visualization(model, test_loader):\n",
    "\n",
    "  data_iter = iter(test_loader)\n",
    "  images, labels = data_iter.next() # obtain one batch of test images\n",
    "  images = images.to(device=device, dtype=torch.float32)\n",
    "  labels = labels.to(device=device, dtype=torch.long)\n",
    "  with torch.no_grad():\n",
    "      output = model(images) # get model output\n",
    "  _, preds = torch.max(output, 1) # convert output probabilities to predicted class\n",
    "  images = images.cpu().numpy() # prep images for display\n",
    "  # plot the images in the batch, along with predicted and true labels\n",
    "  fig = plt.figure(figsize=(25, 4))\n",
    "  for idx in np.arange(20):\n",
    "      ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "      ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "      ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())), color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "executionInfo": {
     "elapsed": 182823,
     "status": "ok",
     "timestamp": 1601150930323,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "osfUv94NaEi4",
    "outputId": "b1ec8c03-0497-4eab-e420-a608ecd0967f"
   },
   "outputs": [],
   "source": [
    "visualization(cnn_1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pbH6LzoQ9i5"
   },
   "source": [
    "Let us extract predicted (*preds*) and ground truth (*targets*) labels for images arising from the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxxIS6e6KXrY"
   },
   "outputs": [],
   "source": [
    "def get_all_prediction(model, loader):\n",
    "    preds = torch.tensor([], dtype=torch.long)\n",
    "    targets = torch.tensor([], dtype=torch.long)\n",
    "    for data, label in loader:\n",
    "        data = data.to(device=device, dtype=torch.float32)\n",
    "        label = label.to(device=device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "        targets = torch.cat((targets, label.cpu()), dim = 0)\n",
    "        preds = torch.cat((preds, torch.max(output.cpu(), 1)[1]), dim = 0)\n",
    "    return targets.numpy(), preds.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8hqY6sX64Nm"
   },
   "outputs": [],
   "source": [
    "targets, preds_1 = get_all_prediction(cnn_1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tq8IGvWn67Rs"
   },
   "source": [
    "#### **Question 5.1** - Visualize some wrongly predicted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 184595,
     "status": "ok",
     "timestamp": 1601150932129,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "-dcFjx1QGpkm",
    "outputId": "1908a110-1ce1-4089-c2eb-fef35fccf890"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwTFkR2RX-dx"
   },
   "source": [
    "#### **Question 5.2** - Display the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zw0oafsMX-dx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "class_names= ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='confusion matrix', cmap=plt.cm.Blues):\n",
    "    # This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"normalized confusion matrix\")\n",
    "    else:\n",
    "        print('confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 184992,
     "status": "ok",
     "timestamp": 1601150932556,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "nkuaLoa-8jOz",
    "outputId": "7785ad57-861e-47bf-c1ac-8a5b61d29743"
   },
   "outputs": [],
   "source": [
    "# compute confusion matrix\n",
    "cnf_matrix = \n",
    "\n",
    "\n",
    "# plot normalized confusion matrix\n",
    "\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9AcBqA5qQ7h"
   },
   "source": [
    "### 6 - Robustness to noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nEFKytdqvPz"
   },
   "outputs": [],
   "source": [
    "class AddGaussianNoise(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.std = std\n",
    "        self.mean = mean\n",
    "        \n",
    "    def __call__(self, tensor):\n",
    "        return tensor + torch.randn(tensor.size()) * self.std + self.mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztLqmxHORxj6"
   },
   "source": [
    "Image transformations can be chained together using *Compose* ([documentation](https://pytorch.org/docs/stable/torchvision/transforms.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Kpks3nVq0lQ"
   },
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    AddGaussianNoise(0., 0.8)\n",
    "])\n",
    "\n",
    "train_data = datasets.MNIST(root = 'data', train = True, download = True, transform = transform)\n",
    "test_data = datasets.MNIST(root = 'data', train = False, download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqaBxTnLq5nQ"
   },
   "outputs": [],
   "source": [
    "train_loader_n, valid_loader_n, test_loader_n = create_data_loaders(batch_size, valid_size, train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atrFtOHzSEAM"
   },
   "source": [
    "Let us visualize some noisy images with corresponding ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "executionInfo": {
     "elapsed": 186034,
     "status": "ok",
     "timestamp": 1601150933635,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "IaIAC26CsH9R",
    "outputId": "775ac7d9-5a72-4bc5-8d22-86071b2dc012"
   },
   "outputs": [],
   "source": [
    "data_iter = iter(train_loader_n) \n",
    "images, labels = data_iter.next() # obtain one batch from the train set\n",
    "images = images.numpy()\n",
    "# plot images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray') # .npsqueeze removes single-dimensional entries from the shape of an array\n",
    "    ax.set_title(str(labels[idx].item())) # .item() gets the value contained in a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_GIdzfq2RccF"
   },
   "source": [
    "#### **Question 6.1** - Train the network on noisy data and evaluate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 544
    },
    "executionInfo": {
     "elapsed": 425740,
     "status": "ok",
     "timestamp": 1601151173359,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "4uNLhPjRsZom",
    "outputId": "8810d8b2-f279-4299-fb96-483b8dbc8611"
   },
   "outputs": [],
   "source": [
    "train_losses_n, valid_losses_n = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 425723,
     "status": "ok",
     "timestamp": 1601151173360,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "gGsSOrTJtAL8",
    "outputId": "71cce560-5fd9-400b-d880-23b45cea5bf2"
   },
   "outputs": [],
   "source": [
    "cnn_1.load_state_dict(torch.load('model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 427793,
     "status": "ok",
     "timestamp": 1601151175449,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "qfz9omj3tGhD",
    "outputId": "a9533d8d-cdf9-4170-f4fe-4e861ad1eb89"
   },
   "outputs": [],
   "source": [
    "evaluation(...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mpq2aqtw7MdR"
   },
   "source": [
    "### 7- Towards deeper CNN models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FY_2gW_3SPGY"
   },
   "source": [
    "In this last part, we come back to the original dataset, i.e. without additional noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9vHjuf_ST3q"
   },
   "source": [
    "#### **Question 7.1** - Implement a deeper convolutional neural network by adding two convolutional layers, one max pooling layer as well as dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i27yTyfnQj1t"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net2(nn.Module): \n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net2,self).__init__()\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "executionInfo": {
     "elapsed": 427769,
     "status": "ok",
     "timestamp": 1601151175451,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "TiddhzR3SkW-",
    "outputId": "c163ac47-f9d7-4a1d-e864-f450ccfd0609"
   },
   "outputs": [],
   "source": [
    "cnn_2 = Net2()\n",
    "cnn_2.to(device=device)\n",
    "\n",
    "summary(...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUP7EYwn-9lC"
   },
   "outputs": [],
   "source": [
    "optimizer = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "executionInfo": {
     "elapsed": 650304,
     "status": "ok",
     "timestamp": 1601151398013,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "xkfZoZzxStIn",
    "outputId": "5cbd9922-20b2-4857-f5bb-5359c29cb77a"
   },
   "outputs": [],
   "source": [
    "train_losses_2, valid_losses_2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 650288,
     "status": "ok",
     "timestamp": 1601151398014,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "xp_M2Lw-1L6L",
    "outputId": "65df802a-17c0-412c-9f28-ce5604ca2f91"
   },
   "outputs": [],
   "source": [
    "cnn_2.load_state_dict(torch.load('model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3KyDic8UbmE"
   },
   "source": [
    "#### **Question 7.2** - Quantify the gain from *Net1* to *Net2* in term of overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 652435,
     "status": "ok",
     "timestamp": 1601151400179,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "Y6-N59GizdA9",
    "outputId": "f9b1b7ca-6b75-498c-8778-842f9bdde4e3"
   },
   "outputs": [],
   "source": [
    "evaluation(...) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HGAp_PrunNf"
   },
   "source": [
    "#### **Question 7.3** - What are the results for incorrect predictions arising from *Net1*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10BGrED1z6qH"
   },
   "outputs": [],
   "source": [
    "_, preds_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYHp_7VYwuC7"
   },
   "source": [
    "You may display test images and labels in the following format: predicted from *Net1* (ground-truth) | predicted from *Net2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 654479,
     "status": "ok",
     "timestamp": 1601151402249,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "s_Yeqh9p95ti",
    "outputId": "35febc56-c642-4bf0-f0e9-b0351a4a8271"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 4))\n",
    "for i in range(20):\n",
    "  plt.subplot(2, 10, i + 1)\n",
    "  plt.axis('off')\n",
    "  plt.imshow(test_set_array[index[i],:,:], cmap='gray')\n",
    "  plt.title(\"{} ({}) | {}\".format(str(np.int(preds_1[index[i]])), str(np.int(targets[index[i]])), str(np.int(preds_2[index[i]]))), color=(\"green\" if preds_2[index[i]]==targets[index[i]] else \"red\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSu3G7LWWcde"
   },
   "source": [
    "### 8- Interactive demonstration\n",
    "\n",
    "To finish this lab session, let us play with the following interactive demo: [link](http://mnist-demo.herokuapp.com). This web application demonstrates the ability of both CNN and MLP models to classify handwritten digits.\n",
    "\n",
    "#### **Question 8.1** - Try to draw a digit which is more accurately classified by CNN than MLP. \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab-session-4-CNN-MNIST-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
