{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE2F5Cj5X-cq"
   },
   "source": [
    "# Lab session 3 | Multi-Layer Perceptron (MLP) with pytorch\n",
    "\n",
    "pierre-henri.conze@imt-atlantique.fr \\\\\n",
    "francois.rousseau@imt-atlantique.fr \\\\\n",
    "simon.benaichouche@imt-atlantique.fr \\\\\n",
    "aurelien.colin@imt-atlantique.fr\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZNAqdzCX-cs"
   },
   "source": [
    "## Objective of this lab session: use a multiple layer perceptron to process the MNIST dataset.\n",
    "\n",
    "\n",
    "**MNIST** is a computer vision dataset. It consists of images of handwritten digits. It also includes labels for each image, telling us which digit it is. In this lab session, we're going to train **Multi-Layer Perceptron** (MLP) models to look at images and predict to which digit they belong."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85qr0BEYX-cu"
   },
   "source": [
    "### 1- Download MNIST dataset\n",
    "\n",
    "First, start with these lines of code which will automatically download the MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 709,
     "status": "ok",
     "timestamp": 1601362670575,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "W6fsgjPOX-cv",
    "outputId": "4e0aed3a-208f-446b-fa9e-488eff9f7bf7"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.ToTensor() # convert data to torch.FloatTensor\n",
    "\n",
    "train_data = datasets.MNIST(root = 'data', train = True, download = True, transform = transform)\n",
    "test_data = datasets.MNIST(root = 'data', train = False, download = True, transform = transform)\n",
    "\n",
    "num_train, num_test = len(train_data), len(test_data)\n",
    "print('number of training data:', num_train, '\\n'+'number of test data:', num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wl7qqo-045pO"
   },
   "source": [
    "The MNIST dataset is split into two parts: 60,000 samples for training, 10,000 samples for test. Both training set test sets contain images and their corresponding labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FQKjqImJ4mKx"
   },
   "source": [
    "The following cell creates a **data loader** (DataLoader, [documentation](https://pytorch.org/docs/stable/data.html)) for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ho-I57ync1Ya"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "batch_size = 20 # how many samples per batch to load\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "18G47IQQX-c3"
   },
   "source": [
    "### 2- Data visualization\n",
    "\n",
    "The first thing to do in a classification task is to take a **look at the data**, make sure that it has been correctly loaded in, then make any initial observations about patterns in that data. The next cell allows to visualize images from the test set with corresponding ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "executionInfo": {
     "elapsed": 1399,
     "status": "ok",
     "timestamp": 1601362677056,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "arAFBf_q0K3p",
    "outputId": "ea3b9349-6499-478e-c2d3-489bbf062aa0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "data_iter = iter(test_loader) \n",
    "images, labels = data_iter.next() # obtain one batch from the test set\n",
    "images = images.numpy()\n",
    "# plot images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n",
    "    ax.imshow(np.squeeze(images[idx]), cmap='gray') # .npsqueeze removes single-dimensional entries from the shape of an array\n",
    "    ax.set_title(str(labels[idx].item())) # .item() gets the value contained in a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BO-rPwr05H2L"
   },
   "source": [
    "Every image in MNIST is of a handwritten digit between zero and nine. Let us view one of these images with more details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592
    },
    "executionInfo": {
     "elapsed": 3838,
     "status": "ok",
     "timestamp": 1601362748720,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "dZEH94l05Ids",
    "outputId": "cf44dab2-9ee3-474e-c77e-0994417937f7"
   },
   "outputs": [],
   "source": [
    "img = np.squeeze(images[1])\n",
    "fig = plt.figure(figsize = (10,10)) \n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(img, cmap='gray')\n",
    "width, height = img.shape\n",
    "thresh = img.max()/2.5\n",
    "for x in range(width):\n",
    "    for y in range(height):\n",
    "        val = round(img[x][y],1) if img[x][y] !=0 else 0\n",
    "        ax.annotate(str(val), xy=(y,x), horizontalalignment='center', verticalalignment='center', color='white' if img[x][y]<thresh else 'black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HA1W3RtT5hnv"
   },
   "source": [
    "Each image is 28 pixels by 28 pixels. Image intensities are normalized between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXXkFECRnxFi"
   },
   "source": [
    "### 3- Is my network generalizing well?\n",
    "\n",
    "It's essential in machine (or deep) learning to consider validation/test sets which will not be used for training such that we can be sure that what we have learned through the network actually generalizes!\n",
    "\n",
    "In practice, we will extract from the training set a subset (20% of training set) for **validation** purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pAZK2_NuDbK3"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "valid_size = 0.2 # percentage of training set to use as validation\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_index, valid_index = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_index)\n",
    "valid_sampler = SubsetRandomSampler(valid_index)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = train_sampler)\n",
    "valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9vqCd8jpJnE"
   },
   "source": [
    "### 4- Simple MLP architecture\n",
    "\n",
    "Let us define a simple **network architecture**. The network will take as inputs 784-dimensional tensors of pixel values from each image (28x28=784). It will produce as output a tensor of length 10 (i.e. the number of classes) that indicates the class scores for each input image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "897aqt3UyfHz"
   },
   "source": [
    "In this first network architecture (*Net1*), we will employ a single hidden layer with 10 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4lLiWMnC5jBy"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net1(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net1,self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 10) # linear layer (784 -> 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(-1,28*28) # flatten input image\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnJCZtJ9x-wA"
   },
   "source": [
    "For a given image, we aim at computing the probabilities for each digit through a softmax regression model $ y = softmax(Wx+b)$ where *softmax* is the normalized exponential function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 7009,
     "status": "ok",
     "timestamp": 1600949712455,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "OFwVrc_XT3cs",
    "outputId": "e9f20298-fd6b-495d-ae00-24274e5b6574"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device '+str(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bo_8WGGk5F31"
   },
   "source": [
    "Let us **initialize** the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 8981,
     "status": "ok",
     "timestamp": 1600949714445,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "JbdKfY3h-wAy",
    "outputId": "7b719866-0ede-40f4-8136-54fa477b5ba8"
   },
   "outputs": [],
   "source": [
    "model_1 = Net1() # initialize the neural network\n",
    "model_1.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eq3-Q0ZtyL0P"
   },
   "source": [
    "### 5- Loss function and optimizer\n",
    "\n",
    "Now, We have to specify the **loss function**. We try to minimize that error: the smaller the error margin, the better our model. In this lab session, the cross-entropy is used as loss function:\n",
    "\n",
    "$$H_{y}(z) = - \\sum_i y_i \\log(z_i)$$\n",
    "\n",
    "where $z$ is our predicted probability distribution, and $y$ is the true distribution. If you look at the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), you can see that PyTorch’s cross entropy function applies a *softmax* funtion to the output layer and then calculates the log loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BeZr3uLL58Qw"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss() # specify loss function (categorical cross-entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP16MCJD03wD"
   },
   "source": [
    "We also need to specify the **optimization** algorithm that will be used to minimized the loss function. Here, we will use a standard stochastic gradient descent (SGD). Many other optimization strategies can be emloyed ([documentation](https://pytorch.org/docs/stable/optim.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_IkVZB-01tJ"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model_1.parameters(),lr = 0.01) # specify optimizer (stochastic gradient descent) and learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dxYFf5sG0_XC"
   },
   "source": [
    "### 6- Network training\n",
    "\n",
    "Let us train the network defined above. Each step is described in the comments below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TbGZO4wt6jTR"
   },
   "outputs": [],
   "source": [
    "n_epochs = 30 # number of epochs to train the model\n",
    "\n",
    "def training(n_epochs, train_loader, valid_loader, model, criterion, optimizer):\n",
    "\n",
    "  train_losses, valid_losses = [], []\n",
    "  # initialize tracker for minimum validation loss\n",
    "  valid_loss_min = np.Inf  # set initial \"min\" to infinity\n",
    "\n",
    "  for epoch in range(n_epochs):\n",
    "      train_loss, valid_loss = 0, 0 # monitor losses\n",
    "      \n",
    "      # train the model\n",
    "      model.train() # prep model for training\n",
    "      for data, label in train_loader:\n",
    "          data = data.to(device=device, dtype=torch.float32)\n",
    "          label = label.to(device=device, dtype=torch.long)\n",
    "          optimizer.zero_grad() # clear the gradients of all optimized variables\n",
    "          output = model(data) # forward pass: compute predicted outputs by passing inputs to the model\n",
    "          loss = criterion(output, label) # calculate the loss\n",
    "          loss.backward() # backward pass: compute gradient of the loss with respect to model parameters\n",
    "          optimizer.step() # perform a single optimization step (parameter update)\n",
    "          train_loss += loss.item() * data.size(0) # update running training loss\n",
    "      \n",
    "      # validate the model\n",
    "      model.eval()\n",
    "      for data, label in valid_loader:\n",
    "          data = data.to(device=device, dtype=torch.float32)\n",
    "          label = label.to(device=device, dtype=torch.long)\n",
    "          with torch.no_grad():\n",
    "              output = model(data)\n",
    "          loss = criterion(output,label)\n",
    "          valid_loss += loss.item() * data.size(0)\n",
    "      \n",
    "      # calculate average loss over an epoch\n",
    "      train_loss /= len(train_loader.sampler)\n",
    "      valid_loss /= len(valid_loader.sampler)\n",
    "      train_losses.append(train_loss)\n",
    "      valid_losses.append(valid_loss)\n",
    "      \n",
    "      print('epoch: {} \\ttraining Loss: {:.6f} \\tvalidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n",
    "\n",
    "      # save model if validation loss has decreased\n",
    "      if valid_loss <= valid_loss_min:\n",
    "          print('validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "          valid_loss_min,\n",
    "          valid_loss))\n",
    "          torch.save(model.state_dict(), 'model.pt')\n",
    "          valid_loss_min = valid_loss\n",
    "      \n",
    "  return train_losses, valid_losses      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eFVXs_h4HR6s"
   },
   "source": [
    "Labels as considered as **one-hot vectors**. A one-hot vector is a vector which is 0 in most dimensions, and 1 in a single dimension. In this case, the n$^{th}$ digit is represented as a vector which is 1 in the n$^{th}$ dimension. For example, label 3 corresponds to: [0,0,0,1,0,0,0,0,0,0]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 291752,
     "status": "ok",
     "timestamp": 1600949997256,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "fulupe2f71cJ",
    "outputId": "eeb19071-2046-4ac5-fcee-5560572418a9"
   },
   "outputs": [],
   "source": [
    "train_losses_1, valid_losses_1 = training(n_epochs, train_loader, valid_loader, model_1, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "twmMCMSz1NoN"
   },
   "source": [
    "### 7- Convergence\n",
    "\n",
    "To study the **convergence** of the training process, we plot the evolution of the loss function for both training and validation sets with respect to epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 291734,
     "status": "ok",
     "timestamp": 1600949997257,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "HpT1Y7cNBA5F",
    "outputId": "c69d7578-2526-4369-e8f3-4886bbdacc2e"
   },
   "outputs": [],
   "source": [
    "plt.plot(range(n_epochs), train_losses_1)\n",
    "plt.plot(range(n_epochs), valid_losses_1)\n",
    "plt.legend(['train', 'validation'], prop={'size': 10})\n",
    "plt.title('loss function', size=10)\n",
    "plt.xlabel('epoch', size=10)\n",
    "plt.ylabel('loss value', size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1C17bsUC1W_Y"
   },
   "source": [
    "Let us load the model corresponding to the lowest validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 291717,
     "status": "ok",
     "timestamp": 1600949997258,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "qzToZJN3FBbS",
    "outputId": "8c6adf1c-7688-4119-d6a7-0ce65537d9d6"
   },
   "outputs": [],
   "source": [
    "model_1.load_state_dict(torch.load('model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NM5BWPs_3Pbz"
   },
   "source": [
    "### 8- Testing\n",
    "\n",
    "Finally, we test our best model on previously unseen test data and evaluate it’s performance. Testing on unseen data is a good way to check that our model generalizes well! It may also be useful to be granular in this analysis and take a look at how this model performs on each class as well as looking at its **overall** loss and **accuracy**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KQIsKly4D6bw"
   },
   "outputs": [],
   "source": [
    "def evaluation(model, test_loader, criterion):\n",
    "\n",
    "  # initialize lists to monitor test loss and accuracy\n",
    "  test_loss = 0.0\n",
    "  class_correct = list(0. for i in range(10))\n",
    "  class_total = list(0. for i in range(10))\n",
    "\n",
    "  model.eval() # prep model for evaluation\n",
    "  for data, label in test_loader:\n",
    "      data = data.to(device=device, dtype=torch.float32)\n",
    "      label = label.to(device=device, dtype=torch.long)\n",
    "      with torch.no_grad():\n",
    "          output = model(data) # forward pass: compute predicted outputs by passing inputs to the model\n",
    "      loss = criterion(output, label)\n",
    "      test_loss += loss.item()*data.size(0)\n",
    "      _, pred = torch.max(output, 1) # convert output probabilities to predicted class\n",
    "      correct = np.squeeze(pred.eq(label.data.view_as(pred))) # compare predictions to true label\n",
    "      # calculate test accuracy for each object class\n",
    "      for i in range(len(label)):\n",
    "          digit = label.data[i]\n",
    "          class_correct[digit] += correct[i].item()\n",
    "          class_total[digit] += 1\n",
    "\n",
    "  # calculate and print avg test loss\n",
    "  test_loss = test_loss/len(test_loader.sampler)\n",
    "  print('test Loss: {:.6f}\\n'.format(test_loss))\n",
    "  for i in range(10):\n",
    "      print('test accuracy of %1s: %2d%% (%2d/%2d)' % (str(i), 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "  print('\\ntest accuracy (overall): %2.2f%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 294504,
     "status": "ok",
     "timestamp": 1600950000074,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "Dc3tr9mEZ2rP",
    "outputId": "0546533d-552a-401e-8b33-96d00dd49998"
   },
   "outputs": [],
   "source": [
    "evaluation(model_1, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6Qr8y1JrplH"
   },
   "source": [
    "#### **Question 8.1** - What is the most difficult digit to predict? Is overall accuracy good? **Answer** : ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Brj2Tz_6QMU"
   },
   "source": [
    "### 9- Visualize results\n",
    "\n",
    "The following cell displays test images and their labels in this format: predicted (ground-truth). The text is green for accurately classified examples and red for incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7rqBNImBD-E0"
   },
   "outputs": [],
   "source": [
    "def visualization(model, test_loader):\n",
    "\n",
    "  data_iter = iter(test_loader)\n",
    "  images, labels = data_iter.next() # obtain one batch of test images\n",
    "  images = images.to(device=device, dtype=torch.float32)\n",
    "  labels = labels.to(device=device, dtype=torch.long)\n",
    "  with torch.no_grad():\n",
    "      output = model(images) # get model output\n",
    "  _, preds = torch.max(output, 1) # convert output probabilities to predicted class\n",
    "  images = images.cpu().numpy() # prep images for display\n",
    "  # plot the images in the batch, along with predicted and true labels\n",
    "  fig = plt.figure(figsize=(25, 4))\n",
    "  for idx in np.arange(20):\n",
    "      ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "      ax.imshow(np.squeeze(images[idx]), cmap='gray')\n",
    "      ax.set_title(\"{} ({})\".format(str(preds[idx].item()), str(labels[idx].item())), color=(\"green\" if preds[idx]==labels[idx] else \"red\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "executionInfo": {
     "elapsed": 295228,
     "status": "ok",
     "timestamp": 1600950000826,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "osfUv94NaEi4",
    "outputId": "f26d059f-8cdf-43cb-deae-f856b13b50fb"
   },
   "outputs": [],
   "source": [
    "visualization(model_1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVeueqWq6kU4"
   },
   "source": [
    "### 10- Incorrect predictions\n",
    "\n",
    "First of all, let us extract predicted (*preds*) and ground truth (*targets*) labels for images arising from the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jxxIS6e6KXrY"
   },
   "outputs": [],
   "source": [
    "def get_all_prediction(model, loader):\n",
    "    preds = torch.tensor([], dtype=torch.long)\n",
    "    targets = torch.tensor([], dtype=torch.long)\n",
    "    for data, label in loader:\n",
    "        data = data.to(device=device, dtype=torch.float32)\n",
    "        label = label.to(device=device, dtype=torch.long)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "        targets = torch.cat((targets, label.cpu()), dim = 0)\n",
    "        preds = torch.cat((preds, torch.max(output.cpu(), 1)[1]), dim = 0)\n",
    "    return targets.numpy(), preds.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g8hqY6sX64Nm"
   },
   "outputs": [],
   "source": [
    "targets, preds_1 = get_all_prediction(model_1, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tq8IGvWn67Rs"
   },
   "source": [
    "We are now ready to visualize some of the incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 297541,
     "status": "ok",
     "timestamp": 1600950003172,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "-dcFjx1QGpkm",
    "outputId": "f59a56ef-25bd-47e6-de17-fe89fb31b5b4"
   },
   "outputs": [],
   "source": [
    "index = np.where(preds_1 - targets != 0)[0]\n",
    "test_set_array = test_data.data.numpy()\n",
    "plt.figure(figsize=(25, 4))\n",
    "for i in range(20):\n",
    "  plt.subplot(2, 10, i + 1)\n",
    "  plt.axis('off')\n",
    "  plt.imshow(test_set_array[index[i],:,:], cmap='gray')\n",
    "  plt.title(\"{} ({})\".format(str(np.int(preds_1[index[i]])), str(np.int(targets[index[i]]))),color=(\"red\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwTFkR2RX-dx"
   },
   "source": [
    "### 11- Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22L0r1zEtJqR"
   },
   "source": [
    "The confusion matrix is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix corresponds to the instances in a ground truth class while each column represents the instances in an predicted class (or vice versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zw0oafsMX-dx"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "class_names= ['0','1','2','3','4','5','6','7','8','9']\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='confusion matrix', cmap=plt.cm.Blues):\n",
    "    # This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"normalized confusion matrix\")\n",
    "    else:\n",
    "        print('confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('true label')\n",
    "    plt.xlabel('predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 299186,
     "status": "ok",
     "timestamp": 1600950004845,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "nkuaLoa-8jOz",
    "outputId": "f04d5e00-b76b-48f8-ac6e-eacbe18aff8e"
   },
   "outputs": [],
   "source": [
    "# compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(targets, preds_1)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# plot normalized confusion matrix\n",
    "plt.figure(figsize=(6, 6))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title='normalized confusion matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uAlWW7lsbDQ"
   },
   "source": [
    "#### **Question 11.1** - What are the most common errors? **Answer**: ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mpq2aqtw7MdR"
   },
   "source": [
    "### 12- Deeper MLP\n",
    "\n",
    "Our goal is to define a new architecture and train the network to reach (at least) 97% of global accuracy for the test dataset.\n",
    "\n",
    "#### **Question 12.1** - Define a deeper network architecture. In practice, we may use two hidden layers (with 512 neurons) as well as dropout ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html)) to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i27yTyfnQj1t"
   },
   "outputs": [],
   "source": [
    "class Net2(nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(Net2,self).__init__()\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "executionInfo": {
     "elapsed": 299161,
     "status": "ok",
     "timestamp": 1600950004847,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "TiddhzR3SkW-",
    "outputId": "c6d72c04-0607-4f5e-c9ff-bf132fe0d967"
   },
   "outputs": [],
   "source": [
    "model_2 = Net2()\n",
    "model_2.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvfwuG2b7toM"
   },
   "source": [
    "#### **Question 12.2** - Train the new MLP model (*Net2*)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define an optimizer for model_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUP7EYwn-9lC"
   },
   "outputs": [],
   "source": [
    "optimizer = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the \"training\" function in Section 6 to train the model_2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 986
    },
    "executionInfo": {
     "elapsed": 655961,
     "status": "ok",
     "timestamp": 1600950361675,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "xkfZoZzxStIn",
    "outputId": "964a3bf0-0563-40fd-f1e2-995df3501898"
   },
   "outputs": [],
   "source": [
    "train_losses_2, valid_losses_2 = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sO9E6UY8H3ab"
   },
   "source": [
    "#### **Question 12.3** - For comparison purposes, display in a single graph the loss functions for both training and validation sets and both models (*Net1* and *Net2*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 310
    },
    "executionInfo": {
     "elapsed": 655944,
     "status": "ok",
     "timestamp": 1600950361676,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "3BVzXDIS9JzI",
    "outputId": "503414f6-448e-4a64-9ec0-9ae9df8773ff"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(['train m1', 'validation m1', 'train m2', 'validation m2'], prop={'size': 10})\n",
    "plt.title('loss function', size=10)\n",
    "plt.xlabel('epoch', size=10)\n",
    "plt.ylabel('loss value', size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0nkNOcBuIfk"
   },
   "source": [
    "#### **Question 12.4** - Perform the quantitative assessment by computing per-class and overall accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 655927,
     "status": "ok",
     "timestamp": 1600950361677,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "67BAmgUs9XaX",
    "outputId": "bdedd141-b90f-48dc-c868-7a8f69f1382c"
   },
   "outputs": [],
   "source": [
    "model_2.load_state_dict(torch.load('model.pt', map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the Evaluation function defined in Section 8 to assess the performance of model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 658619,
     "status": "ok",
     "timestamp": 1600950364389,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "CqBiPair9Y27",
    "outputId": "d1f81fd8-66c4-4d0f-bac9-4a6eec53f7d5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6wq-olf_86Jn"
   },
   "source": [
    "#### **Question 12.5** - Perform the qualitative assessment by displaying some test images with their predicted and ground truth labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "executionInfo": {
     "elapsed": 659448,
     "status": "ok",
     "timestamp": 1600950365239,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "1ZGhHSuO9i5_",
    "outputId": "091f5397-5d9c-4af3-8874-009d7c6bedc3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "94BcUYIG9Vv_"
   },
   "source": [
    "#### **Question 12.6** - Compute the predicted labels for the test set and display the resulting confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wyh-y8w290Nf"
   },
   "outputs": [],
   "source": [
    "_, preds_2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "executionInfo": {
     "elapsed": 661542,
     "status": "ok",
     "timestamp": 1600950367360,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "kbuIhyq_uXQ8",
    "outputId": "a29b6445-69fb-432b-ca53-2cd4aaa4de9b"
   },
   "outputs": [],
   "source": [
    "# compute confusion matrix\n",
    "cnf_matrix = \n",
    "\n",
    "# plot normalized confusion matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HGAp_PrunNf"
   },
   "source": [
    "#### **Question 12.7** - What are the results for incorrect predictions arising from *Net1*? Labels should be indicated in the following format: predicted from *Net1* (ground-truth) | predicted from *Net2*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 662706,
     "status": "ok",
     "timestamp": 1600950368542,
     "user": {
      "displayName": "Pierre-Henri Conze",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhZ4Qbz7lM98QP5Y2jxN8hIcfomTY44lTm-4fcHOg=s64",
      "userId": "10921126790911374273"
     },
     "user_tz": -120
    },
    "id": "s_Yeqh9p95ti",
    "outputId": "1f0e3b0c-d10c-4239-a1fc-2caa14c2fe5b"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MqNAXRUN9il8"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "lab-session-3-MLP-MNIST-correction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
