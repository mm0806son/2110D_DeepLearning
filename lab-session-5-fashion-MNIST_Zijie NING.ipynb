{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"lab-session-5-fashion-MNIST-student.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"cells":[{"cell_type":"markdown","metadata":{"id":"mE2F5Cj5X-cq"},"source":["# Graded lab session 5 | Deep classification on Fashion-MNIST using pytorch\n","\n","pierre-henri.conze@imt-atlantique.fr \\\\\n","francois.rousseau@imt-atlantique.fr \\\\\n","aurelien.colin@imt-atlantique.fr \\\\\n","simon.benaichouche@imt-atlantique.fr\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pJIDcT7uH6Pb"},"source":["#### **Student name** = [TO BE COMPLETED] "]},{"cell_type":"markdown","metadata":{"id":"aZNAqdzCX-cs"},"source":["### Objective of this lab session: perform classification on Fashion-MNIST using multi-layer perceptron, convolutional neural networks and transfer learning to obtain the best classification results as possible."]},{"cell_type":"markdown","metadata":{"id":"hFDfkShgOzbv"},"source":["#### Challenge: the first 3 students in terms of overall test accuracy results (whatever the methodology used) will obtain bonus points!"]},{"cell_type":"markdown","metadata":{"id":"fG6lx_LHMO0I"},"source":["### Table of Contents\n","\n","* [Data management](#section_1)\n","    * [Question 1.1](#question_1_1)\n","    * [Question 1.2](#question_1_2)\n","    * [Question 1.3](#question_1_3)\n","    * [Question 1.4](#question_1_4)\n","* [Multi-Layer Perceptron](#section_2)    \n","    * [Question 2.1](#question_2_1)\n","    * [Question 2.2](#question_2_2)\n","    * [Question 2.3](#question_2_3)\n","    * [Question 2.4](#question_2_4)\n","    * [Question 2.5](#question_2_5)\n","* [Convolutional neural network](#section_3)    \n","    * [Question 3.1](#question_3_1)\n","    * [Question 3.2](#question_3_2)\n","    * [Question 3.3](#question_3_3)\n","    * [Question 3.4](#question_3_4)\n","    * [Question 3.5](#question_3_5)\n","    * [Question 3.6](#question_3_6)\n","    * [Question 3.7](#question_3_7)\n","* [Transfer learning from ImageNet](#section_4)    \n","    * [Question 4.1](#question_4_1)\n","    * [Question 4.2](#question_4_2)\n","    * [Question 4.3](#question_4_3)\n","    * [Question 4.4](#question_4_4)\n","    * [Question 4.5](#question_4_5)\n","* [Challenge](#section_5)    \n","    * [Question 5.1](#question_5_1)"]},{"cell_type":"markdown","metadata":{"id":"85qr0BEYX-cu"},"source":["## 1- Data management <a class=\"anchor\" id=\"section_1\"></a>\n","\n","Start with these lines of code to automatically download the Fashion-MNIST dataset."]},{"cell_type":"code","metadata":{"id":"W6fsgjPOX-cv"},"source":["from torchvision import datasets\n","import torchvision.transforms as transforms\n","\n","transform = transforms.ToTensor()\n","\n","train_data = datasets.FashionMNIST(root = 'data', train = True, download = True, transform = transform)\n","test_data = datasets.FashionMNIST(root = 'data', train = False, download = True, transform = transform)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qLO0Tf2nQ1wK"},"source":["#### **Question 1.1** - Fashion-MNIST is a dataset consisting of a training set of A examples and a test set of B examples. Each example is a CxC grayscale image, associated with a label from D classes. What are the values for A, B, C and D? <a class=\"anchor\" id=\"question_1_1\"></a>"]},{"cell_type":"code","metadata":{"id":"3M62Fu52Q1IT"},"source":["\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KC63p4FeUSrg"},"source":["As indicated below, each Fashion-MNIST sample is assigned to one of the following classes: 0: T-shirt/top, 1: Trouser, 2: Pullover, 3: Dress, 4: Coat, 5: Sandal, 6: Shirt, 7: Sneaker, 8: Bag, 9: Ankle boot."]},{"cell_type":"code","metadata":{"id":"LmyO8doyTM-K"},"source":["class_names = train_data.classes\n","print(class_names)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FQKjqImJ4mKx"},"source":["#### **Question 1.2** - Complete the following cell to create data loaders ([documentation](https://pytorch.org/docs/stable/data.html)) for training, validation and test sets. <a class=\"anchor\" id=\"question_1_2\"></a>"]},{"cell_type":"code","metadata":{"id":"Ho-I57ync1Ya"},"source":["import torch\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import numpy as np\n","\n","batch_size = 20\n","valid_size = 0.2\n","\n","def create_data_loaders(batch_size, valid_size, train_data, test_data): # FUNCTION TO BE COMPLETED\n","\n","\n","    \n","    \n","    \n","    \n","    \n","  return train_loader, valid_loader, test_loader"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gXWfzusFrlmQ"},"source":["train_loader, valid_loader, test_loader = create_data_loaders(batch_size, valid_size, train_data, test_data) # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"18G47IQQX-c3"},"source":["#### **Question 1.3** - Visualize some images from the training set with corresponding ground truth labels. <a class=\"anchor\" id=\"question_1_3\"></a>"]},{"cell_type":"code","metadata":{"id":"arAFBf_q0K3p"},"source":["# CELL TO BE COMPLETED\n","import matplotlib.pyplot as plt\n","import numpy as np\n","%matplotlib inline\n","\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JKCVHh7UVPXq"},"source":["#### **Question 1.4** - View one of these images with more details by superimposing the greyscale intensity values directly on it. <a class=\"anchor\" id=\"question_1_4\"></a>"]},{"cell_type":"code","metadata":{"id":"LNGyb39UVZrA"},"source":["# CELL TO BE COMPLETED\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzueYKsUXETn"},"source":["## 2- Multi-Layer Perceptron (MLP) <a class=\"anchor\" id=\"section_2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"EUPtjgPfNgRI"},"source":["First, let us use a multi-layer perceptron (MLP) to automatically classify Fashion-MNIST images."]},{"cell_type":"markdown","metadata":{"id":"U0TzO_b4L8ie"},"source":["#### **Question 2.1** - Define a MPL architecture with (at least) 2 fully-connected as well as dropout layers. <a class=\"anchor\" id=\"question_2_1\"></a>"]},{"cell_type":"code","metadata":{"id":"dINfBNnyWtia"},"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MLP(nn.Module): \n","    def __init__(self): # FUNCTION TO BE COMPLETED\n","        super(MLP,self).__init__()\n"," \n","\n","\n","\n","    def forward(self,x): # FUNCTION TO BE COMPLETED\n","\n","        \n","        \n","        \n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NO-JtjSqMfYd"},"source":["#### **Question 2.2** - Train the pre-defined MPL network with cross entropy as loss function and stochastic gradient descent as optimization algorithm. <a class=\"anchor\" id=\"question_2_2\"></a>"]},{"cell_type":"code","metadata":{"id":"giapBUp2WyDP"},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device '+str(device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cNo6rTeSW0tu"},"source":["model_1 = MLP()\n","model_1.to(device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JDzdk4tCW3B0"},"source":["#DEFINE THE CRITERION\n","criterion =         # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7CbSk112W40e"},"source":["#DEFINE THE OPTIMIZER\n","optimizer =         # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3o9SW-i5W7OU"},"source":["n_epochs = 30\n","\n","def training(n_epochs, train_loader, valid_loader, model, criterion, optimizer): # FUNCTION TO BE COMPLETED\n","\n","  train_losses, valid_losses = [], []\n","  valid_loss_min = np.Inf\n","\n","  for epoch in range(n_epochs):\n","      train_loss, valid_loss = 0, 0\n","      \n","      model.train()\n","      for data, label in train_loader:\n","          data = data.to(device=device, dtype=torch.float32)\n","          label = label.to(device=device, dtype=torch.long)\n","          #TO BE COMPLETED\n","          #...  \n","            \n","            \n","            \n","            \n","            \n","      model.eval()\n","      for data, label in valid_loader:\n","          data = data.to(device=device, dtype=torch.float32)\n","          label = label.to(device=device, dtype=torch.long)\n","          #TO BE COMPLETED\n","          #...  \n","\n","            \n","            \n","      train_loss /= len(train_loader.sampler)\n","      valid_loss /= len(valid_loader.sampler)\n","      train_losses.append(train_loss)\n","      valid_losses.append(valid_loss)\n","      \n","      print('epoch: {} \\ttraining Loss: {:.6f} \\tvalidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n","\n","      if valid_loss <= valid_loss_min:\n","          print('validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","          valid_loss_min,\n","          valid_loss))\n","          torch.save(model.state_dict(), 'model.pt')\n","          valid_loss_min = valid_loss\n","      \n","  return train_losses, valid_losses      "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgB8EpTNW_z3"},"source":["#RUN THE TRAINING FUNCTION\n","train_losses_1, valid_losses_1 = # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x6aBlyHlNQY6"},"source":["#### **Question 2.3** - Plot the evolution of the loss function for both training and validation sets with respect to epochs to study the convergence of the training process. <a class=\"anchor\" id=\"question_2_3\"></a>"]},{"cell_type":"code","metadata":{"id":"R351wDBMNOn1"},"source":["# CELL TO BE COMPLETED\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pzHWwaVBOSvk"},"source":["Let us load the model corresponding to the lowest validation loss."]},{"cell_type":"code","metadata":{"id":"s6t9aHVqOVXX"},"source":["model_1.load_state_dict(torch.load('model.pt', map_location=device))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SugKLtJ3OtnD"},"source":["#### **Question 2.4** - Test the best model on (unseen) test data and evaluate itâ€™s performance in terms of overall and per-class accuracy scores. <a class=\"anchor\" id=\"question_2_4\"></a>"]},{"cell_type":"code","metadata":{"id":"aPHP7KUzOa4W"},"source":["def evaluation(model, test_loader, criterion): \n","\n","  test_loss = 0.0\n","  class_correct = list(0. for i in range(10))\n","  class_total = list(0. for i in range(10))\n","\n","  model.eval()\n","  for data, label in test_loader:\n","      data = data.to(device=device, dtype=torch.float32)\n","      label = label.to(device=device, dtype=torch.long)\n","      with torch.no_grad():\n","          output = model(data)\n","      loss = criterion(output, label)\n","      test_loss += loss.item()*data.size(0)\n","      _, pred = torch.max(output, 1)\n","      correct = np.squeeze(pred.eq(label.data.view_as(pred)))\n","      for i in range(len(label)):\n","          digit = label.data[i]\n","          class_correct[digit] += correct[i].item()\n","          class_total[digit] += 1\n","\n","  test_loss = test_loss/len(test_loader.sampler)\n","  print('test Loss: {:.6f}\\n'.format(test_loss))\n","  for i in range(10):\n","      print('test accuracy of %s: %2d%% (%2d/%2d)' % (class_names[i], 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n","  print('\\ntest accuracy (overall): %2.2f%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8zrXIcS0OiSm"},"source":["#RUN THE EVALUATION FUNCTION\n","evaluation() # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q1WGJUCGPa4E"},"source":["#### **Question 2.5** - Visualize some incorrect predictions by displaying test images and labels in the format: \"predicted (ground-truth)\". <a class=\"anchor\" id=\"question_2_5\"></a>"]},{"cell_type":"code","metadata":{"id":"pXY4sJHpPjHu"},"source":["def get_all_prediction(model, loader): \n","    preds = torch.tensor([], dtype=torch.long)\n","    targets = torch.tensor([], dtype=torch.long)\n","    for data, label in loader:\n","        data = data.to(device=device, dtype=torch.float32)\n","        label = label.to(device=device, dtype=torch.long)\n","        with torch.no_grad():\n","            output = model(data)\n","        targets = torch.cat((targets, label.cpu()), dim = 0)\n","        preds = torch.cat((preds, torch.max(output.cpu(), 1)[1]), dim = 0)\n","    return targets.numpy(), preds.numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O166AkLrPlcW"},"source":["#RUN THE GET_ALL_PREDICTION FUNCTION\n","targets, preds_1 =  # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wPXUvq89PpJs"},"source":["# CELL TO BE COMPLETED\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Cxu0TluQyzr"},"source":["## 3- Convolutional neural network (CNN) <a class=\"anchor\" id=\"section_3\"></a>"]},{"cell_type":"markdown","metadata":{"id":"wGj9H4C2qB9o"},"source":["Let us now switch from MLP to convolutional neural network (CNN) to classify Fashion-MNIST images."]},{"cell_type":"markdown","metadata":{"id":"3hBIkaLZqPT8"},"source":["#### **Question 3.1** - Define a CNN architecture using (at least) 2 patterns [CONV - CONV - POOL] as well as dropout and fully-connected layers. <a class=\"anchor\" id=\"question_3_1\"></a>"]},{"cell_type":"code","metadata":{"id":"vwKgvoRzo8U8"},"source":["class CNN(nn.Module):\n","\n","    def __init__(self): # FUNCTION TO BE COMPLETED\n","        super(CNN,self).__init__()\n","        \n","\n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","    def forward(self,x): # FUNCTION TO BE COMPLETED\n","\n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        \n","        return out"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_0ugLPcq85z"},"source":["#### **Question 3.2** - Describe input/output sizes of each layer using summary() from torchsummary. <a class=\"anchor\" id=\"question_3_2\"></a>"]},{"cell_type":"code","metadata":{"id":"leLg7Xq4pWjK"},"source":["from torchsummary import summary\n","\n","model_2 = CNN()\n","model_2.to(device=device)\n","\n","# CALL THE SUMMARY FUNCTION TO DISPLAY THE ARCHITECTURE\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bj1lASDJrLFt"},"source":["#### **Question 3.3** - Train the pre-defined CNN network with cross entropy as loss function and stochastic gradient descent as optimization algorithm. <a class=\"anchor\" id=\"question_3_3\"></a>"]},{"cell_type":"code","metadata":{"id":"GuzMhjqwpeo-"},"source":["#DEFINE THE OPTIMIZER\n","optimizer =         # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w6LQ523QpmVY"},"source":["#RUN THE TRAINING FUNCTION\n","train_losses_2, valid_losses_2 =  # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VENh7bUYrrhP"},"source":["#### **Question 3.4** - Display in a single graph the loss functions for training and validation sets obtained with both MLP and CNN models. <a class=\"anchor\" id=\"question_3_4\"></a>"]},{"cell_type":"code","metadata":{"id":"GvRvEpx4r3Ow"},"source":["# CELL TO BE COMPLETED\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1C17bsUC1W_Y"},"source":["As previously, we load the model corresponding to the lowest validation loss."]},{"cell_type":"code","metadata":{"id":"qzToZJN3FBbS"},"source":["model_2.load_state_dict(torch.load('model.pt', map_location=device))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qOEIMqQ7scl4"},"source":["#### **Question 3.5** - Test the model on test data and evaluate it through overall and per-class accuracy scores. Compare these scores with the ones obtained using MLP. <a class=\"anchor\" id=\"question_3_5\"></a>"]},{"cell_type":"code","metadata":{"id":"xX3jtWoxsmmc"},"source":["#RUN THE EVALUATION FUNCTION\n","evaluation(   ) # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sZvnzHYOv3bs"},"source":["#### **Question 3.6** - Display CNN results for images whose prediction was incorrect with MLP. Labels should be indicated in the following format: \"CNN label (ground-truth)\" in red if wrong, green otherwise. <a class=\"anchor\" id=\"question_3_6\"></a>"]},{"cell_type":"code","metadata":{"id":"zczfDPAVvux_"},"source":["#RUN THE GET_ALL_PREDICTION FUNCTION\n","targets, preds_2 =  # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUzvJDOZv_lK"},"source":["# CELL TO BE COMPLETED\n","\n","\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NwWEBlcwvLrW"},"source":["#### **Question 3.7** - Display the confusion matrix for both MLP and CNN models. <a class=\"anchor\" id=\"question_3_7\"></a>"]},{"cell_type":"code","metadata":{"id":"UTp-Kcn5vAz2"},"source":["from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","def plot_confusion_matrix(cm, classes, normalize=False, title='confusion matrix', cmap=plt.cm.Blues):\n","    # This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"normalized confusion matrix\")\n","    else:\n","        print('confusion matrix, without normalization')\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('true label')\n","    plt.xlabel('predicted label')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBzaFZDhvCid"},"source":["# CELL TO BE COMPLETED\n","# compute confusion matrix\n","cnf_matrix_MLP = \n","cnf_matrix_CNN = \n","np.set_printoptions(precision=2)\n","\n","# plot normalized confusion matrixes\n","plt.figure(figsize=(14, 6))\n","plt.subplot(1, 2, 1)\n","plot_confusion_matrix(cnf_matrix_MLP, classes=class_names, normalize=True, title='MLP')\n","plt.subplot(1, 2, 2)\n","plot_confusion_matrix(cnf_matrix_CNN, classes=class_names, normalize=True, title='CNN')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WbWiLIy2Oaxt"},"source":["## 4- Transfer learning from ImageNet <a class=\"anchor\" id=\"section_4\"></a>"]},{"cell_type":"markdown","metadata":{"id":"2HD_zB8sOnfr"},"source":["Transfer learning focuses on storing knowledge gained while solving one problem and applying it to a different but related problem."]},{"cell_type":"markdown","metadata":{"id":"V_SLC0WRUuEG"},"source":["Our goal is to use a (well-known) CNN architecture whose weights have been already trained on ImageNet to perform transfer learning and fine tuning. ImageNet is a large visual database designed for visual object recognition purposes wich contains more than 14 million images!"]},{"cell_type":"markdown","metadata":{"id":"f9b6NafpUq8P"},"source":["#### **Question 4.1** - Modify the Fashion-MNIST dataset to make it fit to  ImageNet by extending Fashion-MNIST images from greyscale to 3-channels images. Since the minimal image dimension as inputs of pre-trained CNN models is 48x48, you will also need to resize images. To do so, use transform.Compose(), transforms.Resize() and transforms.Grayscale(). More details on image transformations are available in the [documentation](https://pytorch.org/docs/stable/torchvision/transforms.html). <a class=\"anchor\" id=\"question_4_1\"></a>"]},{"cell_type":"code","metadata":{"id":"Il44BSBeQS2I"},"source":["#CALL transforms.Compose to adapt the dataset to the VGG16 architecture\n","transform =  # TO DO\n","\n","train_data = datasets.FashionMNIST(root = 'data', train = True, download = True, transform = transform)\n","test_data = datasets.FashionMNIST(root = 'data', train = False, download = True, transform = transform)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jigD05BKXbSu"},"source":["#### **Question 4.2** - Create training, validation and test data loaders. Check the dimensions of images arising from the modified Fasion-MNIST dataset. <a class=\"anchor\" id=\"question_4_2\"></a>"]},{"cell_type":"code","metadata":{"id":"81GaRvq4Pisu"},"source":["# CELL TO BE COMPLETED\n","train_loader, valid_loader, test_loader =    #TO DO\n","data_iter =   #TO DO\n","images, labels = data_iter.next()\n","print(images.numpy().shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b3j7Kt8BPPoC"},"source":["#### **Question 4.3** - Exploit a vgg16 architecture ([related paper](https://arxiv.org/pdf/1409.1556.pdf)) pre-trained on ImageNet to improve the classification scores on Fashion-MNIST through fine-tuning. See the [documentation](https://pytorch.org/docs/stable/torchvision/models.html) for further details. Use summary() from torchsummary to describe each of the vgg16 layers. To avoid a too long learning time, you may use only 5 epochs during fine-tuning. <a class=\"anchor\" id=\"question_4_3\"></a>"]},{"cell_type":"code","metadata":{"id":"1Vkf-UjXPis4"},"source":["import torchvision.models as models\n","\n","model_3 =  # TO DO\n","model_3.to(device=device)\n","summary(model_3, (3, 48, 48))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bt2NYRJERihH"},"source":["#DEFINE THE OPTIMIZER\n","optimizer =         # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hdffyHTmR8MH"},"source":["n_epochs = 5\n","#RUN THE TRAINING FUNCTION\n","train_losses_3, valid_losses_3 =     # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"acJYM6QpYVZf"},"source":["#### **Question 4.4** - Test the model on test data and evaluate it through overall and per-class accuracy scores. Compare these scores with the previously obtained ones. <a class=\"anchor\" id=\"question_4_4\"></a>"]},{"cell_type":"code","metadata":{"id":"y03a3KIXYEN-"},"source":["model_3.load_state_dict(torch.load('model.pt', map_location=device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rGaCM59eYHsZ"},"source":["#RUN THE EVALUATION FUNCTION\n","evaluation(   ) # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"di4DPxghSxis"},"source":["#### **Question 4.5** - Display results for images whose prediction was incorrect with MLP. Labels should be indicated in the following format: \"vgg16 label (ground-truth)\" in red if wrong, green otherwise. <a class=\"anchor\" id=\"question_4_5\"></a>"]},{"cell_type":"code","metadata":{"id":"DbqsJZtKYtcQ"},"source":["#RUN THE GET_ALL_PREDICTION FUNCTION\n","targets, preds_3 =    # TO DO"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTYP6F6DYxrz"},"source":["# CELL TO BE COMPLETED\n","\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"raI8_sq9HqTp"},"source":["## 5- Challenge <a class=\"anchor\" id=\"section_5\"></a>"]},{"cell_type":"markdown","metadata":{"id":"w9bUJzMzEfoj"},"source":["#### **Question 5.1** - What is your best overall test accuracy and with which methodology? The first 3 teams will obtain bonus points! <a class=\"anchor\" id=\"question_5_1\"></a>"]}]}