{"cells":[{"cell_type":"markdown","metadata":{"id":"mE2F5Cj5X-cq"},"source":["# Graded lab session 5 | Deep classification on Fashion-MNIST using pytorch\n","\n","pierre-henri.conze@imt-atlantique.fr \\\\\n","francois.rousseau@imt-atlantique.fr \\\\\n","aurelien.colin@imt-atlantique.fr \\\\\n","simon.benaichouche@imt-atlantique.fr\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pJIDcT7uH6Pb"},"source":["#### **Student name** = David Hurtado & Zijie NING"]},{"cell_type":"markdown","metadata":{"id":"aZNAqdzCX-cs"},"source":["### Objective of this lab session: perform classification on Fashion-MNIST using multi-layer perceptron, convolutional neural networks and transfer learning to obtain the best classification results as possible."]},{"cell_type":"markdown","metadata":{"id":"hFDfkShgOzbv"},"source":["#### Challenge: the first 3 students in terms of overall test accuracy results (whatever the methodology used) will obtain bonus points!"]},{"cell_type":"markdown","metadata":{"id":"fG6lx_LHMO0I"},"source":["### Table of Contents\n","\n","* [Data management](#section_1)\n","    * [Question 1.1](#question_1_1)\n","    * [Question 1.2](#question_1_2)\n","    * [Question 1.3](#question_1_3)\n","    * [Question 1.4](#question_1_4)\n","* [Multi-Layer Perceptron](#section_2)    \n","    * [Question 2.1](#question_2_1)\n","    * [Question 2.2](#question_2_2)\n","    * [Question 2.3](#question_2_3)\n","    * [Question 2.4](#question_2_4)\n","    * [Question 2.5](#question_2_5)\n","* [Convolutional neural network](#section_3)    \n","    * [Question 3.1](#question_3_1)\n","    * [Question 3.2](#question_3_2)\n","    * [Question 3.3](#question_3_3)\n","    * [Question 3.4](#question_3_4)\n","    * [Question 3.5](#question_3_5)\n","    * [Question 3.6](#question_3_6)\n","    * [Question 3.7](#question_3_7)\n","* [Transfer learning from ImageNet](#section_4)    \n","    * [Question 4.1](#question_4_1)\n","    * [Question 4.2](#question_4_2)\n","    * [Question 4.3](#question_4_3)\n","    * [Question 4.4](#question_4_4)\n","    * [Question 4.5](#question_4_5)\n","* [Challenge](#section_5)    \n","    * [Question 5.1](#question_5_1)"]},{"cell_type":"markdown","metadata":{"id":"85qr0BEYX-cu"},"source":["## 1- Data management <a class=\"anchor\" id=\"section_1\"></a>\n","\n","Start with these lines of code to automatically download the Fashion-MNIST dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W6fsgjPOX-cv"},"outputs":[],"source":["from torchvision import datasets\n","import torchvision.transforms as transforms\n","\n","transform = transforms.ToTensor()\n","\n","train_data = datasets.FashionMNIST(root = 'data', train = True, download = True, transform = transform)\n","test_data = datasets.FashionMNIST(root = 'data', train = False, download = True, transform = transform)"]},{"cell_type":"markdown","metadata":{"id":"qLO0Tf2nQ1wK"},"source":["#### **Question 1.1** - Fashion-MNIST is a dataset consisting of a training set of A examples and a test set of B examples. Each example is a CxC grayscale image, associated with a label from D classes. What are the values for A, B, C and D? <a class=\"anchor\" id=\"question_1_1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3M62Fu52Q1IT"},"outputs":[],"source":["num_train, num_test = len(train_data), len(test_data)\n","print('A = number of training data =', num_train, '\\n'+'B = number of test data =', num_test)\n","#images, labels = data_iter.next() # obtain one batch from the train set\n","#images = images.numpy()\n","# ? num_pixel = len(np.squeeze(images[1]))\n","num_pixel = 28\n","num_class = len(train_data.classes)\n","print('C = number of pixel =', num_pixel, '\\n'+'D = number of class =', num_class)\n"]},{"cell_type":"markdown","metadata":{"id":"KC63p4FeUSrg"},"source":["As indicated below, each Fashion-MNIST sample is assigned to one of the following classes: 0: T-shirt/top, 1: Trouser, 2: Pullover, 3: Dress, 4: Coat, 5: Sandal, 6: Shirt, 7: Sneaker, 8: Bag, 9: Ankle boot."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LmyO8doyTM-K"},"outputs":[],"source":["class_names = train_data.classes\n","print(class_names)"]},{"cell_type":"markdown","metadata":{"id":"FQKjqImJ4mKx"},"source":["#### **Question 1.2** - Complete the following cell to create data loaders ([documentation](https://pytorch.org/docs/stable/data.html)) for training, validation and test sets. <a class=\"anchor\" id=\"question_1_2\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ho-I57ync1Ya"},"outputs":[],"source":["import torch\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import numpy as np\n","\n","batch_size = 20\n","valid_size = 0.2\n","\n","def create_data_loaders(batch_size, valid_size, train_data, test_data): # FUNCTION TO BE COMPLETED\n","  # obtain training indices that will be used for validation\n","  indices = list(range(num_train))\n","  np.random.shuffle(indices)\n","  split = int(np.floor(valid_size * num_train))\n","  train_index, valid_index = indices[split:], indices[:split]\n","\n","  # define samplers for obtaining training and validation batches\n","  train_sampler = SubsetRandomSampler(train_index)\n","  valid_sampler = SubsetRandomSampler(valid_index)\n","\n","  # prepare data loaders\n","  test_loader = torch.utils.data.DataLoader(test_data, batch_size = batch_size)\n","  train_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = train_sampler)\n","  valid_loader = torch.utils.data.DataLoader(train_data, batch_size = batch_size, sampler = valid_sampler)\n","  \n","  return train_loader, valid_loader, test_loader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gXWfzusFrlmQ"},"outputs":[],"source":["train_loader, valid_loader, test_loader = create_data_loaders(batch_size, valid_size, train_data, test_data) # TO DO"]},{"cell_type":"markdown","metadata":{"id":"18G47IQQX-c3"},"source":["#### **Question 1.3** - Visualize some images from the training set with corresponding ground truth labels. <a class=\"anchor\" id=\"question_1_3\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"arAFBf_q0K3p"},"outputs":[],"source":["# CELL TO BE COMPLETED\n","import matplotlib.pyplot as plt\n","import numpy as np\n","%matplotlib inline\n","\n","data_iter = iter(train_loader) \n","images, labels = data_iter.next() # obtain one batch from the train set\n","images = images.numpy()\n","# plot images in the batch, along with the corresponding labels\n","fig = plt.figure(figsize=(25, 4))\n","for idx in np.arange(20):\n","    ax = fig.add_subplot(2, 10, idx+1, xticks=[], yticks=[])\n","    ax.imshow(np.squeeze(images[idx]), cmap='gray') # .npsqueeze removes single-dimensional entries from the shape of an array\n","    ax.set_title(str(labels[idx].item())) # .item() gets the value contained in a Tensor\n"]},{"cell_type":"markdown","metadata":{"id":"JKCVHh7UVPXq"},"source":["#### **Question 1.4** - View one of these images with more details by superimposing the greyscale intensity values directly on it. <a class=\"anchor\" id=\"question_1_4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LNGyb39UVZrA"},"outputs":[],"source":["# CELL TO BE COMPLETED\n","img = np.squeeze(images[1])\n","fig = plt.figure(figsize = (10,10)) \n","ax = fig.add_subplot(111)\n","ax.imshow(img, cmap='gray')\n","width, height = img.shape\n","thresh = img.max()/2.5\n","for x in range(width):\n","    for y in range(height):\n","        val = round(img[x][y],1) if img[x][y] !=0 else 0\n","        ax.annotate(str(val), xy=(y,x), horizontalalignment='center', verticalalignment='center', color='white' if img[x][y]<thresh else 'black')\n"]},{"cell_type":"markdown","metadata":{"id":"xzueYKsUXETn"},"source":["## 2- Multi-Layer Perceptron (MLP) <a class=\"anchor\" id=\"section_2\"></a>"]},{"cell_type":"markdown","metadata":{"id":"EUPtjgPfNgRI"},"source":["First, let us use a multi-layer perceptron (MLP) to automatically classify Fashion-MNIST images."]},{"cell_type":"markdown","metadata":{"id":"U0TzO_b4L8ie"},"source":["#### **Question 2.1** - Define a MPL architecture with (at least) 2 fully-connected as well as dropout layers. <a class=\"anchor\" id=\"question_2_1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dINfBNnyWtia"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class MLP(nn.Module): \n","    def __init__(self): # FUNCTION TO BE COMPLETED\n","        super(MLP,self).__init__()\n","        self.fc1 = nn.Linear(28*28, 14*14) # linear layer (784 -> 10)\n","        self.Drop = nn.Dropout(p=0.2)\n","        self.rel1 = nn.ReLU()\n","        self.fc2 = nn.Linear(14*14, 10) # linear layer (784 -> 10)\n","\n","\n","    def forward(self,x): # FUNCTION TO BE COMPLETED\n","        x = x.view(-1,28*28) # flatten input image\n","        x = self.fc1(x)\n","        x = self.Drop(x)\n","        x = self.rel1(x)\n","        x = self.fc2(x)\n","        \n","        \n","        \n","        return x"]},{"cell_type":"markdown","metadata":{"id":"NO-JtjSqMfYd"},"source":["#### **Question 2.2** - Train the pre-defined MPL network with cross entropy as loss function and stochastic gradient descent as optimization algorithm. <a class=\"anchor\" id=\"question_2_2\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"giapBUp2WyDP"},"outputs":[],"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print('Using device '+str(device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cNo6rTeSW0tu"},"outputs":[],"source":["model_1 = MLP()\n","model_1.to(device=device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDzdk4tCW3B0"},"outputs":[],"source":["#DEFINE THE CRITERION\n","# TO DO\n","criterion = nn.CrossEntropyLoss() # specify loss function (categorical cross-entropy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7CbSk112W40e"},"outputs":[],"source":["#DEFINE THE OPTIMIZER\n","# TO DO\n","optimizer = torch.optim.SGD(model_1.parameters(),lr = 0.01) # specify optimizer (stochastic gradient descent) and learning rate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3o9SW-i5W7OU"},"outputs":[],"source":["n_epochs = 30\n","\n","def training(n_epochs, train_loader, valid_loader, model, criterion, optimizer): # FUNCTION TO BE COMPLETED\n","\n","  train_losses, valid_losses = [], []\n","  valid_loss_min = np.Inf\n","\n","  for epoch in range(n_epochs):\n","      train_loss, valid_loss = 0, 0\n","      # train the model\n","      model.train()\n","      for data, label in train_loader:\n","          data = data.to(device=device, dtype=torch.float32)\n","          label = label.to(device=device, dtype=torch.long)\n","          #TO BE COMPLETED\n","          #...  \n","          optimizer.zero_grad() # clear the gradients of all optimized variables\n","          output = model(data) # forward pass: compute predicted outputs by passing inputs to the model\n","          loss = criterion(output, label) # calculate the loss\n","          loss.backward() # backward pass: compute gradient of the loss with respect to model parameters\n","          optimizer.step() # perform a single optimization step (parameter update)\n","          train_loss += loss.item() * data.size(0) # update running training loss\n","      \n","      # validate the model      \n","      model.eval()\n","      for data, label in valid_loader:\n","          data = data.to(device=device, dtype=torch.float32)\n","          label = label.to(device=device, dtype=torch.long)\n","          #TO BE COMPLETED\n","          #...  \n","          with torch.no_grad():\n","            output = model(data)\n","          loss = criterion(output,label)\n","          valid_loss += loss.item() * data.size(0)\n","            \n","            \n","      train_loss /= len(train_loader.sampler)\n","      valid_loss /= len(valid_loader.sampler)\n","      train_losses.append(train_loss)\n","      valid_losses.append(valid_loss)\n","      \n","      print('epoch: {} \\ttraining Loss: {:.6f} \\tvalidation Loss: {:.6f}'.format(epoch+1, train_loss, valid_loss))\n","\n","      if valid_loss <= valid_loss_min:\n","          print('validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","          valid_loss_min,\n","          valid_loss))\n","          torch.save(model.state_dict(), 'model.pt')\n","          valid_loss_min = valid_loss\n","      \n","  return train_losses, valid_losses      "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BgB8EpTNW_z3"},"outputs":[],"source":["#RUN THE TRAINING FUNCTION\n","# TO DO\n","train_losses_1, valid_losses_1 = training(n_epochs, train_loader, valid_loader, model_1, criterion, optimizer)"]},{"cell_type":"markdown","metadata":{"id":"x6aBlyHlNQY6"},"source":["#### **Question 2.3** - Plot the evolution of the loss function for both training and validation sets with respect to epochs to study the convergence of the training process. <a class=\"anchor\" id=\"question_2_3\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R351wDBMNOn1"},"outputs":[],"source":["# CELL TO BE COMPLETED\n","plt.plot(range(n_epochs), train_losses_1)\n","plt.plot(range(n_epochs), valid_losses_1)\n","plt.legend(['train', 'validation'], prop={'size': 10})\n","plt.title('loss function', size=10)\n","plt.xlabel('epoch', size=10)\n","plt.ylabel('loss value', size=10)\n"]},{"cell_type":"markdown","metadata":{"id":"pzHWwaVBOSvk"},"source":["Let us load the model corresponding to the lowest validation loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6t9aHVqOVXX"},"outputs":[],"source":["model_1.load_state_dict(torch.load('model.pt', map_location=device))"]},{"cell_type":"markdown","metadata":{"id":"SugKLtJ3OtnD"},"source":["#### **Question 2.4** - Test the best model on (unseen) test data and evaluate it’s performance in terms of overall and per-class accuracy scores. <a class=\"anchor\" id=\"question_2_4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aPHP7KUzOa4W"},"outputs":[],"source":["def evaluation(model, test_loader, criterion): \n","\n","  test_loss = 0.0\n","  class_correct = list(0. for i in range(10))\n","  class_total = list(0. for i in range(10))\n","\n","  model.eval()\n","  for data, label in test_loader:\n","      data = data.to(device=device, dtype=torch.float32)\n","      label = label.to(device=device, dtype=torch.long)\n","      with torch.no_grad():\n","          output = model(data)\n","      loss = criterion(output, label)\n","      test_loss += loss.item()*data.size(0)\n","      _, pred = torch.max(output, 1)\n","      correct = np.squeeze(pred.eq(label.data.view_as(pred)))\n","      for i in range(len(label)):\n","          digit = label.data[i]\n","          class_correct[digit] += correct[i].item()\n","          class_total[digit] += 1\n","\n","  test_loss = test_loss/len(test_loader.sampler)\n","  print('test Loss: {:.6f}\\n'.format(test_loss))\n","  for i in range(10):\n","      print('test accuracy of %s: %2d%% (%2d/%2d)' % (class_names[i], 100 * class_correct[i] / class_total[i], np.sum(class_correct[i]), np.sum(class_total[i])))\n","  print('\\ntest accuracy (overall): %2.2f%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total), np.sum(class_correct), np.sum(class_total)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8zrXIcS0OiSm"},"outputs":[],"source":["#RUN THE EVALUATION FUNCTION\n","# TO DO\n","evaluation(model_1, test_loader, criterion)"]},{"cell_type":"markdown","metadata":{"id":"q1WGJUCGPa4E"},"source":["#### **Question 2.5** - Visualize some incorrect predictions by displaying test images and labels in the format: \"predicted (ground-truth)\". <a class=\"anchor\" id=\"question_2_5\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pXY4sJHpPjHu"},"outputs":[],"source":["def get_all_prediction(model, loader): \n","    preds = torch.tensor([], dtype=torch.long)\n","    targets = torch.tensor([], dtype=torch.long)\n","    for data, label in loader:\n","        data = data.to(device=device, dtype=torch.float32)\n","        label = label.to(device=device, dtype=torch.long)\n","        with torch.no_grad():\n","            output = model(data)\n","        targets = torch.cat((targets, label.cpu()), dim = 0)\n","        preds = torch.cat((preds, torch.max(output.cpu(), 1)[1]), dim = 0)\n","    return targets.numpy(), preds.numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O166AkLrPlcW"},"outputs":[],"source":["#RUN THE GET_ALL_PREDICTION FUNCTION\n","# TO DO\n","targets, preds_1 = get_all_prediction(model_1, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wPXUvq89PpJs"},"outputs":[],"source":["# CELL TO BE COMPLETED\n","index = np.where(preds_1 - targets != 0)[0]\n","test_set_array = test_data.data.numpy()\n","plt.figure(figsize=(25, 4))\n","for i in range(20):\n","  plt.subplot(2, 10, i + 1)\n","  plt.axis('off')\n","  plt.imshow(test_set_array[index[i],:,:], cmap='gray')\n","  plt.title(\"{} ({})\".format(str(np.int(preds_1[index[i]])), str(np.int(targets[index[i]]))),color=(\"red\"))\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"-Cxu0TluQyzr"},"source":["## 3- Convolutional neural network (CNN) <a class=\"anchor\" id=\"section_3\"></a>"]},{"cell_type":"markdown","metadata":{"id":"wGj9H4C2qB9o"},"source":["Let us now switch from MLP to convolutional neural network (CNN) to classify Fashion-MNIST images."]},{"cell_type":"markdown","metadata":{"id":"3hBIkaLZqPT8"},"source":["#### **Question 3.1** - Define a CNN architecture using (at least) 2 patterns [CONV - CONV - POOL] as well as dropout and fully-connected layers. <a class=\"anchor\" id=\"question_3_1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwKgvoRzo8U8"},"outputs":[],"source":["class CNN(nn.Module):\n","\n","    def __init__(self): # FUNCTION TO BE COMPLETED\n","        super(CNN,self).__init__()\n","        self.conv1 = nn.Conv2d(1,32, kernel_size=3, stride=1) # Convuntional (784 -> 10)\n","        self.conv2 = nn.Conv2d(32,32, kernel_size=3, stride=1) # Convuntional (784 -> 10)\n","        self.relu = nn.ReLU()\n","        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n","        self.fc1 = nn.Linear(12*12*32, 10)\n","        self.Drop = nn.Dropout(p=0.2)\n","        \n","        \n","    def forward(self,x): # FUNCTION TO BE COMPLETED\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.conv2(x)\n","        x = self.relu(x)\n","        x = self.pool1(x)\n","        x = x.view(-1,12*12*32) # flatten input image        \n","        out = self.fc1(x)\n","\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"P_0ugLPcq85z"},"source":["#### **Question 3.2** - Describe input/output sizes of each layer using summary() from torchsummary. <a class=\"anchor\" id=\"question_3_2\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"leLg7Xq4pWjK"},"outputs":[],"source":["from torchsummary import summary\n","\n","model_2 = CNN()\n","model_2.to(device=device)\n","\n","# CALL THE SUMMARY FUNCTION TO DISPLAY THE ARCHITECTURE\n"]},{"cell_type":"markdown","metadata":{"id":"Bj1lASDJrLFt"},"source":["#### **Question 3.3** - Train the pre-defined CNN network with cross entropy as loss function and stochastic gradient descent as optimization algorithm. <a class=\"anchor\" id=\"question_3_3\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GuzMhjqwpeo-"},"outputs":[],"source":["#DEFINE THE OPTIMIZER\n","# TO DO\n","optimizer = torch.optim.SGD(model_2.parameters(),lr = 0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6LQ523QpmVY"},"outputs":[],"source":["#RUN THE TRAINING FUNCTION\n","#train_losses_2, valid_losses_2 =  # TO DO\n","train_losses_2, valid_losses_2 = training(n_epochs, train_loader, valid_loader, model_2, criterion, optimizer)"]},{"cell_type":"markdown","metadata":{"id":"VENh7bUYrrhP"},"source":["#### **Question 3.4** - Display in a single graph the loss functions for training and validation sets obtained with both MLP and CNN models. <a class=\"anchor\" id=\"question_3_4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvRvEpx4r3Ow"},"outputs":[],"source":["# CELL TO BE COMPLETED\n","plt.plot(range(n_epochs), train_losses_1)\n","plt.plot(range(n_epochs), valid_losses_1)\n","plt.plot(range(n_epochs), train_losses_2)\n","plt.plot(range(n_epochs), valid_losses_2)\n","\n","\n","\n","plt.legend(['train MLP', 'validation MLP', 'train CNN', 'validation CNN'], prop={'size': 10})\n","plt.title('loss function', size=10)\n","plt.xlabel('epoch', size=10)\n","plt.ylabel('loss value', size=10)\n"]},{"cell_type":"markdown","metadata":{"id":"1C17bsUC1W_Y"},"source":["As previously, we load the model corresponding to the lowest validation loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzToZJN3FBbS"},"outputs":[],"source":["model_2.load_state_dict(torch.load('model.pt', map_location=device))"]},{"cell_type":"markdown","metadata":{"id":"qOEIMqQ7scl4"},"source":["#### **Question 3.5** - Test the model on test data and evaluate it through overall and per-class accuracy scores. Compare these scores with the ones obtained using MLP. <a class=\"anchor\" id=\"question_3_5\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xX3jtWoxsmmc"},"outputs":[],"source":["#RUN THE EVALUATION FUNCTION\n","#evaluation(   ) # TO DO\n","evaluation(model_2, test_loader, criterion)"]},{"cell_type":"markdown","metadata":{"id":"sZvnzHYOv3bs"},"source":["#### **Question 3.6** - Display CNN results for images whose prediction was incorrect with MLP. Labels should be indicated in the following format: \"CNN label (ground-truth)\" in red if wrong, green otherwise. <a class=\"anchor\" id=\"question_3_6\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zczfDPAVvux_"},"outputs":[],"source":["#RUN THE GET_ALL_PREDICTION FUNCTION\n","#targets, preds_2 =  # TO DO\n","targets, preds_2 = get_all_prediction(model_2, test_loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUzvJDOZv_lK"},"outputs":[],"source":["# CELL TO BE COMPLETED\n","index = np.where(preds_1 - targets != 0)[0]\n","test_set_array = test_data.data.numpy()\n","plt.figure(figsize=(25, 4))\n","\n","for i in range(20):\n","  plt.subplot(2, 10, i + 1)\n","  plt.axis('off')\n","  plt.imshow(test_set_array[index[i],:,:], cmap='gray')\n","  plt.title(\"{} ({})\".format(str(np.int(preds_2[index[i]])), str(np.int(targets[index[i]]))), color=(\"green\" if preds_2[index[i]]==targets[index[i]] else \"red\"))\n","plt.show()\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NwWEBlcwvLrW"},"source":["#### **Question 3.7** - Display the confusion matrix for both MLP and CNN models. <a class=\"anchor\" id=\"question_3_7\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UTp-Kcn5vAz2"},"outputs":[],"source":["from sklearn.metrics import confusion_matrix\n","import itertools\n","\n","def plot_confusion_matrix(cm, classes, normalize=False, title='confusion matrix', cmap=plt.cm.Blues):\n","    # This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"normalized confusion matrix\")\n","    else:\n","        print('confusion matrix, without normalization')\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('true label')\n","    plt.xlabel('predicted label')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WBzaFZDhvCid"},"outputs":[],"source":["# CELL TO BE COMPLETED\n","# compute confusion matrix\n","cnf_matrix_MLP = confusion_matrix(targets, preds_1)\n","cnf_matrix_CNN = confusion_matrix(targets, preds_2)\n","np.set_printoptions(precision=2)\n","\n","# plot normalized confusion matrixes\n","plt.figure(figsize=(14, 6))\n","plt.subplot(1, 2, 1)\n","plot_confusion_matrix(cnf_matrix_MLP, classes=class_names, normalize=True, title='MLP')\n","plt.subplot(1, 2, 2)\n","plot_confusion_matrix(cnf_matrix_CNN, classes=class_names, normalize=True, title='CNN')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"WbWiLIy2Oaxt"},"source":["## 4- Transfer learning from ImageNet <a class=\"anchor\" id=\"section_4\"></a>"]},{"cell_type":"markdown","metadata":{"id":"2HD_zB8sOnfr"},"source":["Transfer learning focuses on storing knowledge gained while solving one problem and applying it to a different but related problem."]},{"cell_type":"markdown","metadata":{"id":"V_SLC0WRUuEG"},"source":["Our goal is to use a (well-known) CNN architecture whose weights have been already trained on ImageNet to perform transfer learning and fine tuning. ImageNet is a large visual database designed for visual object recognition purposes wich contains more than 14 million images!"]},{"cell_type":"markdown","metadata":{"id":"f9b6NafpUq8P"},"source":["#### **Question 4.1** - Modify the Fashion-MNIST dataset to make it fit to  ImageNet by extending Fashion-MNIST images from greyscale to 3-channels images. Since the minimal image dimension as inputs of pre-trained CNN models is 48x48, you will also need to resize images. To do so, use transform.Compose(), transforms.Resize() and transforms.Grayscale(). More details on image transformations are available in the [documentation](https://pytorch.org/docs/stable/torchvision/transforms.html). <a class=\"anchor\" id=\"question_4_1\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Il44BSBeQS2I"},"outputs":[],"source":["#CALL transforms.Compose to adapt the dataset to the VGG16 architecture\n","#transform =  # TO DO\n","transform = transforms.Compose([\n","    transforms.Resize((48, 48)),\n","    #transforms.RandomHorizontalFlip(p=0.5),\n","    #transforms.ColorJitter(brightness=0.5, hue=0.3),\n","    #transforms.RandomAffine(degrees=40, translate=None, scale=(1, 2), shear=15, resample=False, fillcolor=0),\n","    #transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 3)),\n","    transforms.Grayscale(num_output_channels=3),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","])\n","\n","train_data = datasets.FashionMNIST(root = 'data', train = True, download = True, transform = transform)\n","test_data = datasets.FashionMNIST(root = 'data', train = False, download = True, transform = transform)"]},{"cell_type":"markdown","metadata":{"id":"jigD05BKXbSu"},"source":["#### **Question 4.2** - Create training, validation and test data loaders. Check the dimensions of images arising from the modified Fasion-MNIST dataset. <a class=\"anchor\" id=\"question_4_2\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81GaRvq4Pisu"},"outputs":[],"source":["# CELL TO BE COMPLETED\n","# TODO train_loader, valid_loader, test_loader = \n","train_loader, valid_loader, test_loader = create_data_loaders(batch_size, valid_size, train_data, test_data)\n","data_iter = iter(train_loader)   #TO DO\n","images, labels = data_iter.next()\n","print(images.numpy().shape)"]},{"cell_type":"markdown","metadata":{"id":"b3j7Kt8BPPoC"},"source":["#### **Question 4.3** - Exploit a vgg16 architecture ([related paper](https://arxiv.org/pdf/1409.1556.pdf)) pre-trained on ImageNet to improve the classification scores on Fashion-MNIST through fine-tuning. See the [documentation](https://pytorch.org/docs/stable/torchvision/models.html) for further details. Use summary() from torchsummary to describe each of the vgg16 layers. To avoid a too long learning time, you may use only 5 epochs during fine-tuning. <a class=\"anchor\" id=\"question_4_3\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1Vkf-UjXPis4"},"outputs":[],"source":["import torchvision.models as models\n","\n","model_3 = models.vgg16(pretrained=True) # TO DO\n","model_3.to(device=device)\n","summary(model_3, (3, 48, 48))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bt2NYRJERihH"},"outputs":[],"source":["#DEFINE THE OPTIMIZER\n","#optimizer =         # TO DO\n","optimizer = torch.optim.SGD(model_3.parameters(),lr = 0.01)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hdffyHTmR8MH"},"outputs":[],"source":["n_epochs = 5\n","#RUN THE TRAINING FUNCTION\n","#train_losses_3, valid_losses_3 =     # TO DO\n","train_losses_3, valid_losses_3 = training(n_epochs, train_loader, valid_loader, model_3, criterion, optimizer)"]},{"cell_type":"markdown","metadata":{"id":"acJYM6QpYVZf"},"source":["#### **Question 4.4** - Test the model on test data and evaluate it through overall and per-class accuracy scores. Compare these scores with the previously obtained ones. <a class=\"anchor\" id=\"question_4_4\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y03a3KIXYEN-"},"outputs":[],"source":["model_3.load_state_dict(torch.load('model.pt', map_location=device))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rGaCM59eYHsZ"},"outputs":[],"source":["#RUN THE EVALUATION FUNCTION\n","#evaluation(   ) # TO DO\n","evaluation(model_3, test_loader, criterion)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.plot(range(n_epochs), train_losses_3)\n","plt.plot(range(n_epochs), valid_losses_3)\n","\n","plt.legend(['train vgg16', 'validation vgg16'], prop={'size': 10})\n","plt.title('loss function', size=10)\n","plt.xlabel('epoch', size=10)\n","plt.ylabel('loss value', size=10)"]},{"cell_type":"markdown","metadata":{"id":"di4DPxghSxis"},"source":["#### **Question 4.5** - Display results for images whose prediction was incorrect with MLP. Labels should be indicated in the following format: \"vgg16 label (ground-truth)\" in red if wrong, green otherwise. <a class=\"anchor\" id=\"question_4_5\"></a>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DbqsJZtKYtcQ"},"outputs":[],"source":["#RUN THE GET_ALL_PREDICTION FUNCTION\n","#targets, preds_3 =    # TO DO\n","targets, preds_3 = get_all_prediction(model_3, test_loader)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oTYP6F6DYxrz"},"outputs":[],"source":["# CELL TO BE COMPLETED\n","index = np.where(preds_1 - targets != 0)[0]\n","test_set_array = test_data.data.numpy()\n","plt.figure(figsize=(25, 4))\n","\n","for i in range(20):\n","  plt.subplot(2, 10, i + 1)\n","  plt.axis('off')\n","  plt.imshow(test_set_array[index[i],:,:], cmap='gray')\n","  plt.title(\"{} ({})\".format(str(np.int(preds_3[index[i]])), str(np.int(targets[index[i]]))), color=(\"green\" if preds_3[index[i]]==targets[index[i]] else \"red\"))\n","plt.show()\n","\n"]},{"cell_type":"markdown","metadata":{"id":"raI8_sq9HqTp"},"source":["## 5- Challenge <a class=\"anchor\" id=\"section_5\"></a>"]},{"cell_type":"markdown","metadata":{"id":"w9bUJzMzEfoj"},"source":["#### **Question 5.1** - What is your best overall test accuracy and with which methodology? The first 3 teams will obtain bonus points! <a class=\"anchor\" id=\"question_5_1\"></a>\n","\n","**Accuracy: 93.71%**\n","\n","Pretrained VGG16 with lr = 0.01, epoch = 7, \n","\n","Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))"]},{"cell_type":"markdown","metadata":{},"source":["![](https://raw.githubusercontent.com/mm0806son/Images/main/202110291744570.png)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"lab-session-5-fashion-MNIST-student.ipynb","provenance":[]},"interpreter":{"hash":"7b4b4feff2f24a0f0a34464dbe537a36fda679851528fb8735cb41fa49dffb2d"},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
